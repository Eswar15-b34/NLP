{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGCTeqBnR05i7qA1Gwj78K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"d7968048","executionInfo":{"status":"ok","timestamp":1769681445395,"user_tz":-330,"elapsed":657,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"98d6332e-5df0-4ff4-b63f-3739d1f8c896"},"source":["import pandas as pd\n","import re\n","\n","# Load the dataset\n","try:\n","    df = pd.read_csv('/content/Twitter_Data.csv')\n","    print(\"Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: Twitter_Data.csv not found. Please ensure the file is in the /content/ directory.\")\n","    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n","\n","if not df.empty:\n","    # Assuming the tweet text column is named 'text' or 'tweet'. Adjust if necessary.\n","    # Let's inspect the columns to find the most likely text column\n","    print(\"\\nAvailable columns:\", df.columns.tolist())\n","\n","    # Common column names for tweet text are 'text', 'tweet', 'Tweet', 'content', 'message'\n","    # Adding 'clean_text' as a candidate since it was found in the dataframe\n","    text_column_candidates = ['clean_text', 'text', 'tweet', 'Tweet', 'content', 'message']\n","    tweet_text_column = None\n","\n","    for col in text_column_candidates:\n","        if col in df.columns:\n","            tweet_text_column = col\n","            break\n","\n","    if tweet_text_column is None:\n","        print(\"\\nCould not find a common tweet text column. Please identify the correct column from the list above and update the 'tweet_text_column' variable.\")\n","        # If no column is found, we can't proceed with preprocessing\n","    else:\n","        print(f\"\\nUsing '{tweet_text_column}' as the tweet text column.\")\n","        # Display the first 5 rows to confirm data loading\n","        display(df.head())\n","else:\n","    print(\"DataFrame is empty, unable to display.\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded successfully.\n","\n","Available columns: ['clean_text', 'category']\n","\n","Using 'clean_text' as the tweet text column.\n"]},{"output_type":"display_data","data":{"text/plain":["                                          clean_text  category\n","0  when modi promised “minimum government maximum...      -1.0\n","1  talk all the nonsense and continue all the dra...       0.0\n","2  what did just say vote for modi  welcome bjp t...       1.0\n","3  asking his supporters prefix chowkidar their n...       1.0\n","4  answer who among these the most powerful world...       1.0"],"text/html":["\n","  <div id=\"df-118eb64a-7a4a-4116-8058-1358dde23444\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-118eb64a-7a4a-4116-8058-1358dde23444')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-118eb64a-7a4a-4116-8058-1358dde23444 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-118eb64a-7a4a-4116-8058-1358dde23444');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"DataFrame is empty, unable to display\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk all the nonsense and continue all the drama will vote for modi \",\n          \"answer who among these the most powerful world leader today trump putin modi may \",\n          \"what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8944271909999157,\n        \"min\": -1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.0,\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f06653d","executionInfo":{"status":"ok","timestamp":1769681500071,"user_tz":-330,"elapsed":855,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"7c029e26-0b26-4932-9c42-de9cfb88fa7b"},"source":["import pandas as pd\n","import re\n","\n","# Load the dataset\n","try:\n","    df = pd.read_csv('/content/Twitter_Data.csv')\n","    print(\"Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: Twitter_Data.csv not found. Please ensure the file is in the /content/ directory.\")\n","    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n","\n","if not df.empty:\n","    # Assuming the tweet text column is named 'text' or 'tweet'. Adjust if necessary.\n","    # Let's inspect the columns to find the most likely text column\n","    print(\"\\nAvailable columns:\", df.columns.tolist())\n","\n","    # Common column names for tweet text are 'text', 'tweet', 'Tweet', 'content', 'message'\n","    # Adding 'clean_text' as a candidate since it was found in the dataframe\n","    text_column_candidates = ['clean_text', 'text', 'tweet', 'Tweet', 'content', 'message']\n","    tweet_text_column = None\n","\n","    for col in text_column_candidates:\n","        if col in df.columns:\n","            tweet_text_column = col\n","            break\n","\n","    if tweet_text_column is None:\n","        print(\"\\nCould not find a common tweet text column. Please identify the correct column from the list above and update the 'tweet_text_column' variable.\")\n","        # If no column is found, we can't proceed with preprocessing\n","    else:\n","        print(f\"\\nUsing '{tweet_text_column}' as the tweet text column.\")\n","\n","        # Function to remove URLs\n","        def remove_urls(text):\n","            url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","            return url_pattern.sub(r'', text)\n","\n","        # Function to remove mentions\n","        def remove_mentions(text):\n","            mention_pattern = re.compile(r'@\\w+')\n","            return mention_pattern.sub(r'', text)\n","\n","        # Apply preprocessing\n","        print(\"\\nPreprocessing tweet text...\")\n","        df['cleaned_tweet_text'] = df[tweet_text_column].astype(str).apply(remove_urls)\n","        df['cleaned_tweet_text'] = df['cleaned_tweet_text'].apply(remove_mentions)\n","\n","        print(\"\\nOriginal tweet examples:\")\n","        print(df[tweet_text_column].head())\n","        print(\"\\nCleaned tweet examples:\")\n","        print(df['cleaned_tweet_text'].head())\n","        print(\"\\nPreprocessing complete. A new column 'cleaned_tweet_text' has been added to the DataFrame.\")\n","else:\n","    print(\"DataFrame is empty, unable to perform preprocessing.\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded successfully.\n","\n","Available columns: ['clean_text', 'category']\n","\n","Using 'clean_text' as the tweet text column.\n","\n","Preprocessing tweet text...\n","\n","Original tweet examples:\n","0    when modi promised “minimum government maximum...\n","1    talk all the nonsense and continue all the dra...\n","2    what did just say vote for modi  welcome bjp t...\n","3    asking his supporters prefix chowkidar their n...\n","4    answer who among these the most powerful world...\n","Name: clean_text, dtype: object\n","\n","Cleaned tweet examples:\n","0    when modi promised “minimum government maximum...\n","1    talk all the nonsense and continue all the dra...\n","2    what did just say vote for modi  welcome bjp t...\n","3    asking his supporters prefix chowkidar their n...\n","4    answer who among these the most powerful world...\n","Name: cleaned_tweet_text, dtype: object\n","\n","Preprocessing complete. A new column 'cleaned_tweet_text' has been added to the DataFrame.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":724},"id":"bae81c09","executionInfo":{"status":"ok","timestamp":1769681729312,"user_tz":-330,"elapsed":184840,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"fd2d953b-c159-4bf1-e927-b21d22f38648"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","\n","# Download necessary NLTK data for POS tagging if not already present\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    print(\"Downloading 'punkt' for NLTK...\")\n","    nltk.download('punkt')\n","    print(\"'punkt' downloaded.\")\n","\n","try:\n","    # Corrected resource name to 'averaged_perceptron_tagger'\n","    nltk.data.find('taggers/averaged_perceptron_tagger')\n","except LookupError:\n","    print(\"Downloading 'averaged_perceptron_tagger' for NLTK...\")\n","    nltk.download('averaged_perceptron_tagger')\n","    print(\"'averaged_perceptron_tagger' downloaded.\")\n","\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    print(\"Downloading 'wordnet' for NLTK...\")\n","    nltk.download('wordnet')\n","    print(\"'wordnet' downloaded.\")\n","\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    print(\"Downloading 'stopwords' for NLTK...\")\n","    nltk.download('stopwords')\n","    print(\"'stopwords' downloaded.\")\n","\n","print(\"NLTK and its tagger data are ready.\")\n","\n","# Function to perform POS tagging\n","def pos_tag_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    return nltk.pos_tag(tokens)\n","\n","# Function to convert NLTK POS tags to WordNet POS tags\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN # Default to noun if not found\n","\n","# Initialize lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to lemmatize and remove stopwords\n","def lemmatize_and_remove_stopwords(pos_tags):\n","    lemmatized_tokens = []\n","    for word, tag in pos_tags:\n","        # Convert to lowercase and remove non-alphabetic characters\n","        cleaned_word = ''.join(filter(str.isalpha, word)).lower()\n","        if cleaned_word and cleaned_word not in stop_words:\n","            wntag = get_wordnet_pos(tag)\n","            lemmatized_tokens.append(lemmatizer.lemmatize(cleaned_word, wntag))\n","    return ' '.join(lemmatized_tokens)\n","\n","\n","if not df.empty and 'cleaned_tweet_text' in df.columns:\n","    print(\"\\nPerforming POS tagging on 'cleaned_tweet_text'...\")\n","    df['pos_tags'] = df['cleaned_tweet_text'].astype(str).apply(pos_tag_text)\n","    print(\"POS tagging complete. A new column 'pos_tags' has been added to the DataFrame.\")\n","    print(\"\\nExamples of POS tags:\")\n","    display(df[['cleaned_tweet_text', 'pos_tags']].head())\n","\n","    print(\"\\nPerforming lemmatization and stopword removal...\")\n","    df['lemmatized_text'] = df['pos_tags'].apply(lemmatize_and_remove_stopwords)\n","    print(\"Lemmatization and stopword removal complete. A new column 'lemmatized_text' has been added.\")\n","    print(\"\\nExamples of lemmatized text:\")\n","    display(df[['cleaned_tweet_text', 'pos_tags', 'lemmatized_text']].head())\n","else:\n","    print(\"DataFrame is empty or 'cleaned_tweet_text' column not found. Cannot perform POS tagging, lemmatization or stopword removal.\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 'wordnet' for NLTK...\n","'wordnet' downloaded.\n","NLTK and its tagger data are ready.\n","\n","Performing POS tagging on 'cleaned_tweet_text'...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["POS tagging complete. A new column 'pos_tags' has been added to the DataFrame.\n","\n","Examples of POS tags:\n"]},{"output_type":"display_data","data":{"text/plain":["                                  cleaned_tweet_text  \\\n","0  when modi promised “minimum government maximum...   \n","1  talk all the nonsense and continue all the dra...   \n","2  what did just say vote for modi  welcome bjp t...   \n","3  asking his supporters prefix chowkidar their n...   \n","4  answer who among these the most powerful world...   \n","\n","                                            pos_tags  \n","0  [(when, WRB), (modi, NN), (promised, VBD), (“,...  \n","1  [(talk, NN), (all, PDT), (the, DT), (nonsense,...  \n","2  [(what, WP), (did, VBD), (just, RB), (say, VB)...  \n","3  [(asking, VBG), (his, PRP$), (supporters, NNS)...  \n","4  [(answer, NN), (who, WP), (among, IN), (these,...  "],"text/html":["\n","  <div id=\"df-d7090041-ee7f-4292-bde2-56f797e08d0f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_tweet_text</th>\n","      <th>pos_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>[(when, WRB), (modi, NN), (promised, VBD), (“,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>[(talk, NN), (all, PDT), (the, DT), (nonsense,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>[(what, WP), (did, VBD), (just, RB), (say, VB)...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>[(asking, VBG), (his, PRP$), (supporters, NNS)...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>[(answer, NN), (who, WP), (among, IN), (these,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7090041-ee7f-4292-bde2-56f797e08d0f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d7090041-ee7f-4292-bde2-56f797e08d0f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d7090041-ee7f-4292-bde2-56f797e08d0f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"DataFrame is empty or 'cleaned_tweet_text' column not found\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk all the nonsense and continue all the drama will vote for modi \",\n          \"answer who among these the most powerful world leader today trump putin modi may \",\n          \"what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Performing lemmatization and stopword removal...\n","Lemmatization and stopword removal complete. A new column 'lemmatized_text' has been added.\n","\n","Examples of lemmatized text:\n"]},{"output_type":"display_data","data":{"text/plain":["                                  cleaned_tweet_text  \\\n","0  when modi promised “minimum government maximum...   \n","1  talk all the nonsense and continue all the dra...   \n","2  what did just say vote for modi  welcome bjp t...   \n","3  asking his supporters prefix chowkidar their n...   \n","4  answer who among these the most powerful world...   \n","\n","                                            pos_tags  \\\n","0  [(when, WRB), (modi, NN), (promised, VBD), (“,...   \n","1  [(talk, NN), (all, PDT), (the, DT), (nonsense,...   \n","2  [(what, WP), (did, VBD), (just, RB), (say, VB)...   \n","3  [(asking, VBG), (his, PRP$), (supporters, NNS)...   \n","4  [(answer, NN), (who, WP), (among, IN), (these,...   \n","\n","                                     lemmatized_text  \n","0  modi promise minimum government maximum govern...  \n","1             talk nonsense continue drama vote modi  \n","2  say vote modi welcome bjp tell rahul main camp...  \n","3  ask supporter prefix chowkidar name modi great...  \n","4  answer among powerful world leader today trump...  "],"text/html":["\n","  <div id=\"df-5699fc1f-6161-4da9-b6fa-d9abf5fc1441\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_tweet_text</th>\n","      <th>pos_tags</th>\n","      <th>lemmatized_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>[(when, WRB), (modi, NN), (promised, VBD), (“,...</td>\n","      <td>modi promise minimum government maximum govern...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>[(talk, NN), (all, PDT), (the, DT), (nonsense,...</td>\n","      <td>talk nonsense continue drama vote modi</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>[(what, WP), (did, VBD), (just, RB), (say, VB)...</td>\n","      <td>say vote modi welcome bjp tell rahul main camp...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>[(asking, VBG), (his, PRP$), (supporters, NNS)...</td>\n","      <td>ask supporter prefix chowkidar name modi great...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>[(answer, NN), (who, WP), (among, IN), (these,...</td>\n","      <td>answer among powerful world leader today trump...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5699fc1f-6161-4da9-b6fa-d9abf5fc1441')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5699fc1f-6161-4da9-b6fa-d9abf5fc1441 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5699fc1f-6161-4da9-b6fa-d9abf5fc1441');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"DataFrame is empty or 'cleaned_tweet_text' column not found\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk all the nonsense and continue all the drama will vote for modi \",\n          \"answer who among these the most powerful world leader today trump putin modi may \",\n          \"what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk nonsense continue drama vote modi\",\n          \"answer among powerful world leader today trump putin modi may\",\n          \"say vote modi welcome bjp tell rahul main campaigner modi think modi relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecb35325","executionInfo":{"status":"ok","timestamp":1769681892523,"user_tz":-330,"elapsed":6557,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"0bce8fcd-574f-4001-a105-eec9aba4b8e5"},"source":["from collections import defaultdict\n","\n","if not df.empty and 'pos_tags' in df.columns:\n","    print(\"\\nBuilding HMM parameters...\")\n","\n","    # Initialize dictionaries for HMM parameters with Laplace smoothing (add 1)\n","    initial_probabilities = defaultdict(lambda: 1)\n","    transition_probabilities = defaultdict(lambda: defaultdict(lambda: 1))\n","    emission_probabilities = defaultdict(lambda: defaultdict(lambda: 1))\n","    tag_counts = defaultdict(lambda: 0) # To normalize probabilities\n","\n","    all_tags_set = set() # To collect all unique tags\n","\n","    # Process each tweet's POS tags\n","    for tags_sequence in df['pos_tags']:\n","        if tags_sequence:\n","            # Initial probabilities\n","            first_tag = tags_sequence[0][1] # (word, tag) -> tag\n","            initial_probabilities[first_tag] += 1\n","            all_tags_set.add(first_tag)\n","\n","            # Transition and Emission probabilities\n","            for i, (word, tag) in enumerate(tags_sequence):\n","                tag_counts[tag] += 1\n","                all_tags_set.add(tag)\n","                emission_probabilities[tag][word.lower()] += 1 # Use lowercase for words\n","\n","                if i > 0:\n","                    prev_tag = tags_sequence[i-1][1]\n","                    transition_probabilities[prev_tag][tag] += 1\n","\n","    # Convert tag_counts to a regular dict for easier access/iteration\n","    tag_counts_dict = dict(tag_counts)\n","\n","    # Normalize probabilities\n","    # Initial probabilities\n","    total_initial_count = sum(initial_probabilities.values())\n","    initial_probabilities = {tag: count / total_initial_count for tag, count in initial_probabilities.items()}\n","\n","    # Transition probabilities\n","    for prev_tag, next_tag_counts in transition_probabilities.items():\n","        total_transitions_from_prev_tag = sum(next_tag_counts.values())\n","        transition_probabilities[prev_tag] = {next_tag: count / total_transitions_from_prev_tag for next_tag, count in next_tag_counts.items()}\n","\n","    # Emission probabilities\n","    for tag, word_counts in emission_probabilities.items():\n","        total_words_for_tag = sum(word_counts.values())\n","        emission_probabilities[tag] = {word: count / total_words_for_tag for word, count in word_counts.items()}\n","\n","    print(\"HMM parameters built successfully.\")\n","    print(\"\\n--- HMM Parameters Examples ---\")\n","    print(\"\\nInitial Probabilities (top 5):\")\n","    print(dict(list(initial_probabilities.items())[:5]))\n","    print(\"\\nTransition Probabilities (first 3 tags, top 3 transitions each):\")\n","    for i, (prev_tag, next_tag_probs) in enumerate(transition_probabilities.items()):\n","        if i >= 3: break\n","        print(f\"  {prev_tag}: {dict(list(next_tag_probs.items())[:3])}\")\n","    print(\"\\nEmission Probabilities (first 3 tags, top 3 emissions each):\")\n","    for i, (tag, word_probs) in enumerate(emission_probabilities.items()):\n","        if i >= 3: break\n","        print(f\"  {tag}: {dict(list(word_probs.items())[:3])}\")\n","\n","else:\n","    print(\"DataFrame is empty or 'pos_tags' column not found. Cannot build HMM parameters.\")"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Building HMM parameters...\n","HMM parameters built successfully.\n","\n","--- HMM Parameters Examples ---\n","\n","Initial Probabilities (top 5):\n","{'WRB': 0.030844922672981247, 'NN': 0.3353557165555276, 'WP': 0.017520505002791256, 'VBG': 0.019741241281156254, 'JJ': 0.13076578593819974}\n","\n","Transition Probabilities (first 3 tags, top 3 transitions each):\n","  WRB: {'NN': 0.26288186705126526, 'VBZ': 0.028648223416808706, 'JJS': 0.0008522354792185657}\n","  NN: {'VBD': 0.06080158149560789, 'JJ': 0.054530325960094043, 'NNP': 0.008668070206789487}\n","  VBD: {'NNP': 0.0049813780260707635, 'PRP': 0.03550279329608939, 'RB': 0.09728119180633148}\n","\n","Emission Probabilities (first 3 tags, top 3 emissions each):\n","  WRB: {'when': 0.2616143264988321, 'why': 0.351901116013496, 'how': 0.2508434985725409}\n","  NN: {'modi': 0.09749772590027594, 'government': 0.005617408520038831, 'governance': 0.0004959066204967092}\n","  VBD: {'promised': 0.006117570450592479, 'expected': 0.000676874041452109, 'did': 0.06012183732746138}\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"z-0C7luzQx1H"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a69add5","executionInfo":{"status":"ok","timestamp":1769681933854,"user_tz":-330,"elapsed":54,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"296c815a-6673-4798-9f03-eb8954c40f91"},"source":["import math\n","\n","if not df.empty and 'pos_tags' in df.columns and transition_probabilities:\n","    print(\"\\n--- Analyzing Transition Probability Irregularities ---\")\n","\n","    # Helper function to print top N transitions for a given previous tag\n","    def print_top_transitions(prev_tag, num=5):\n","        if prev_tag in transition_probabilities:\n","            sorted_transitions = sorted(transition_probabilities[prev_tag].items(), key=lambda item: item[1], reverse=True)\n","            print(f\"  Top {num} transitions from '{prev_tag}':\")\n","            for next_tag, prob in sorted_transitions[:num]:\n","                print(f\"    -> {next_tag}: {prob:.4f}\")\n","        else:\n","            print(f\"  No transitions found for '{prev_tag}'.\")\n","\n","    print(\"\\nExamples of Transition Probabilities for common tags:\")\n","    print_top_transitions('NN') # Noun\n","    print_top_transitions('VB') # Verb, base form\n","    print_top_transitions('JJ') # Adjective\n","    print_top_transitions('DT') # Determiner\n","    print_top_transitions('IN') # Preposition/subordinating conjunction\n","\n","    # Calculate entropy for transition probabilities\n","    # Entropy = - sum(p * log(p)) for all possible next tags\n","    transition_entropies = {}\n","    for prev_tag, next_tag_probs in transition_probabilities.items():\n","        entropy = 0\n","        for prob in next_tag_probs.values():\n","            if prob > 0: # Avoid log(0)\n","                entropy -= prob * math.log(prob, 2) # Using log base 2 for bits\n","        transition_entropies[prev_tag] = entropy\n","\n","    # Sort tags by entropy to find most/least predictable transitions\n","    sorted_entropies = sorted(transition_entropies.items(), key=lambda item: item[1])\n","\n","    print(\"\\nTags with the lowest transition entropy (most predictable next tags):\")\n","    for tag, entropy in sorted_entropies[:5]:\n","        print(f\"  '{tag}': {entropy:.4f} bits\")\n","        print_top_transitions(tag, num=3)\n","\n","    print(\"\\nTags with the highest transition entropy (least predictable next tags):\")\n","    for tag, entropy in sorted_entropies[-5:]: # Last 5 for highest entropy\n","        print(f\"  '{tag}': {entropy:.4f} bits\")\n","        print_top_transitions(tag, num=3)\n","\n","else:\n","    print(\"HMM parameters (transition_probabilities) not available for analysis.\")"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Analyzing Transition Probability Irregularities ---\n","\n","Examples of Transition Probabilities for common tags:\n","  Top 5 transitions from 'NN':\n","    -> NN: 0.3595\n","    -> IN: 0.0932\n","    -> VBD: 0.0608\n","    -> NNS: 0.0592\n","    -> JJ: 0.0545\n","  Top 5 transitions from 'VB':\n","    -> NN: 0.1905\n","    -> DT: 0.1486\n","    -> JJ: 0.1187\n","    -> PRP$: 0.0864\n","    -> IN: 0.0790\n","  Top 5 transitions from 'JJ':\n","    -> NN: 0.5665\n","    -> NNS: 0.1530\n","    -> JJ: 0.0995\n","    -> IN: 0.0329\n","    -> RB: 0.0208\n","  Top 5 transitions from 'DT':\n","    -> NN: 0.5083\n","    -> JJ: 0.1983\n","    -> NNS: 0.1178\n","    -> JJS: 0.0183\n","    -> RB: 0.0168\n","  Top 5 transitions from 'IN':\n","    -> NN: 0.3217\n","    -> JJ: 0.1733\n","    -> DT: 0.1480\n","    -> NNS: 0.0720\n","    -> PRP: 0.0532\n","\n","Tags with the lowest transition entropy (most predictable next tags):\n","  'SYM': 0.0000 bits\n","  Top 3 transitions from 'SYM':\n","    -> NN: 1.0000\n","  '``': 0.0000 bits\n","  Top 3 transitions from '``':\n","    -> RB: 1.0000\n","  '$': 0.0725 bits\n","  Top 3 transitions from '$':\n","    -> CD: 0.9912\n","    -> RB: 0.0088\n","  'PDT': 0.3935 bits\n","  Top 3 transitions from 'PDT':\n","    -> DT: 0.9409\n","    -> PRP$: 0.0476\n","    -> IN: 0.0059\n","  'POS': 1.2136 bits\n","  Top 3 transitions from 'POS':\n","    -> NN: 0.7750\n","    -> WRB: 0.0750\n","    -> CD: 0.0500\n","\n","Tags with the highest transition entropy (least predictable next tags):\n","  'CC': 3.7163 bits\n","  Top 3 transitions from 'CC':\n","    -> NN: 0.2375\n","    -> JJ: 0.1443\n","    -> VB: 0.0962\n","  'VBD': 3.7220 bits\n","  Top 3 transitions from 'VBD':\n","    -> JJ: 0.1855\n","    -> NN: 0.1557\n","    -> DT: 0.1156\n","  'VBP': 3.7570 bits\n","  Top 3 transitions from 'VBP':\n","    -> JJ: 0.2008\n","    -> NN: 0.1456\n","    -> DT: 0.0897\n","  'VB': 3.7888 bits\n","  Top 3 transitions from 'VB':\n","    -> NN: 0.1905\n","    -> DT: 0.1486\n","    -> JJ: 0.1187\n","  'RB': 3.9364 bits\n","  Top 3 transitions from 'RB':\n","    -> JJ: 0.1841\n","    -> VB: 0.1214\n","    -> RB: 0.0913\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"201c8a97","executionInfo":{"status":"ok","timestamp":1769682054067,"user_tz":-330,"elapsed":2595,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"366c4e54-e53e-46b3-dddf-b54572072735"},"source":["print(\"\\n--- Analyzing Rare and Low-Probability Tokens ---\")\n","\n","# Identify words with the lowest emission probabilities for each tag\n","print(\"\\nTop 5 words with the lowest emission probabilities for each of the first 5 tags:\")\n","for i, (tag, word_probs) in enumerate(emission_probabilities.items()):\n","    if i >= 5: break\n","    if word_probs:\n","        sorted_words = sorted(word_probs.items(), key=lambda item: item[1])\n","        print(f\"  Tag '{tag}':\")\n","        for word, prob in sorted_words[:5]:\n","            print(f\"    '{word}': {prob:.8f}\")\n","    else:\n","        print(f\"  Tag '{tag}': No emission probabilities found.\")\n","\n","# Calculate overall word frequencies to find truly rare words in the dataset\n","word_frequencies = defaultdict(int)\n","for tags_sequence in df['pos_tags']:\n","    if tags_sequence:\n","        for word, _ in tags_sequence:\n","            word_frequencies[word.lower()] += 1\n","\n","# Filter for words that appeared only once (singletons)\n","singletons = {word: count for word, count in word_frequencies.items() if count == 1}\n","\n","print(\"\\nExamples of words that appeared only once in the entire dataset (singletons):\")\n","if singletons:\n","    # Display first 10 singletons\n","    for i, (word, count) in enumerate(list(singletons.items())[:10]):\n","        print(f\"  '{word}' (count: {count})\")\n","        # Check their emission probability if they have one\n","        # Find the tag for this singleton by searching the original pos_tags\n","        found_tags_for_singleton = []\n","        for tags_sequence in df['pos_tags']:\n","            if tags_sequence:\n","                for w, t in tags_sequence:\n","                    if w.lower() == word:\n","                        found_tags_for_singleton.append(t)\n","                        break # Only need one tag for example\n","                if found_tags_for_singleton: break\n","\n","        if found_tags_for_singleton and found_tags_for_singleton[0] in emission_probabilities and word in emission_probabilities[found_tags_for_singleton[0]]:\n","             print(f\"    Emission probability for tag '{found_tags_for_singleton[0]}': {emission_probabilities[found_tags_for_singleton[0]][word]:.8f}\")\n","        else:\n","            print(f\"    Emission probability for tag not found or word not in emission for its tag (due to smoothing, this might be a small default value).\")\n","else:\n","    print(\"  No singletons found.\")\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Analyzing Rare and Low-Probability Tokens ---\n","\n","Top 5 words with the lowest emission probabilities for each of the first 5 tags:\n","  Tag 'WRB':\n","    'mover': 0.00006488\n","    'wasn': 0.00006488\n","    'wld': 0.00006488\n","    'wiselythe': 0.00006488\n","    'whatsapp': 0.00006488\n","  Tag 'NN':\n","    'constituency2': 0.00000191\n","    'tuthukudi': 0.00000191\n","    'thuthukudi': 0.00000191\n","    'leadershipwho': 0.00000191\n","    'modiganga': 0.00000191\n","  Tag 'VBD':\n","    'disabilityif': 0.00001714\n","    'bursted': 0.00001714\n","    'mki': 0.00001714\n","    'stemmed': 0.00001714\n","    'piggybacked': 0.00001714\n","  Tag 'NNP':\n","    '➡': 0.00010847\n","    'िु': 0.00010847\n","    '⚡jai': 0.00010847\n","    'zayed': 0.00010847\n","    'mistakemodi': 0.00010847\n","  Tag 'JJ':\n","    'crustal': 0.00000494\n","    'maarkefir': 0.00000494\n","    'behaved': 0.00000494\n","    'likly': 0.00000494\n","    'insensitivearrogant': 0.00000494\n","\n","Examples of words that appeared only once in the entire dataset (singletons):\n","  'crustal' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'maarkefir' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'tax…the' (count: 1)\n","    Emission probability for tag 'VBP': 0.00001209\n","  'constituency2' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'tuthukudi' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'likly' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'thuthukudi' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'leadershipwho' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'thiugh' (count: 1)\n","    Emission probability for tag 'IN': 0.00000938\n","  'modiganga' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n"]}]},{"cell_type":"code","source":["import math\n","\n","# Function to manually apply Viterbi decoding for a given sequence of words\n","def viterbi_decode(words, initial_prob, transition_prob, emission_prob, all_tags):\n","    \"\"\"\n","    Applies the Viterbi algorithm to find the most likely sequence of hidden states (POS tags)\n","    for a given sequence of observations (words).\n","\n","    Args:\n","        words (list): A list of tokenized words (observations).\n","        initial_prob (dict): Dictionary of initial state probabilities.\n","        transition_prob (dict): Dictionary of transition probabilities (tag_prev -> tag_curr).\n","        emission_prob (dict): Dictionary of emission probabilities (tag -> word).\n","        all_tags (list): A list of all possible POS tags.\n","\n","    Returns:\n","        list: The most likely sequence of POS tags (hidden states) for the input words.\n","    \"\"\"\n","    # Initialize Viterbi path and probabilities table (T)\n","    # T[tag] will store a list of dictionaries, one for each word in the sequence.\n","    # Each dictionary contains the log probability of the best path ending in that tag\n","    # at that position, and the previous tag in that path.\n","    T = {}\n","    for tag in all_tags:\n","        # Calculate log probability for the first word: P(tag) * P(word|tag)\n","        # Use 1e-10 for smoothing to avoid log(0) for unseen events\n","        T[tag] = [{\"prob\": math.log(initial_prob.get(tag, 1e-10)) + math.log(emission_prob.get(tag, {}).get(words[0].lower(), 1e-10)), \"prev\": None}]\n","\n","    # Forward pass: fill the Viterbi table\n","    for i in range(1, len(words)): # Iterate through words from the second word onwards\n","        for current_tag in all_tags: # For each possible current tag\n","            max_log_prob = -float('inf') # Initialize with negative infinity\n","            best_prev_tag = None\n","            # Find the previous tag that leads to the maximum probability path to the current tag\n","            for prev_tag in all_tags:\n","                # Calculate log probability: P(path_to_prev_tag) * P(curr_tag|prev_tag) * P(word_curr|curr_tag)\n","                log_prob = T[prev_tag][-1][\"prob\"] + \\\n","                           math.log(transition_prob.get(prev_tag, {}).get(current_tag, 1e-10)) + \\\n","                           math.log(emission_prob.get(current_tag, {}).get(words[i].lower(), 1e-10))\n","\n","                # Update if a better path is found\n","                if log_prob > max_log_prob:\n","                    max_log_prob = log_prob\n","                    best_prev_tag = prev_tag\n","            # Store the maximum log probability and the best previous tag for the current tag at the current position\n","            T[current_tag].append({\"prob\": max_log_prob, \"prev\": best_prev_tag})\n","\n","    # Find the path with the highest probability at the very end of the sequence\n","    max_log_prob_path = -float('inf')\n","    last_tag_path = None\n","    for tag in all_tags:\n","        if T[tag][-1][\"prob\"] > max_log_prob_path:\n","            max_log_prob_path = T[tag][-1][\"prob\"]\n","            last_tag_path = tag\n","\n","    # Backward pass: reconstruct the best path by backtracking\n","    best_path = [last_tag_path]\n","    # Iterate backwards from the second to last word\n","    for i in range(len(words) - 1, 0, -1):\n","        last_tag_path = T[last_tag_path][i][\"prev\"] # Get the best previous tag\n","        best_path.insert(0, last_tag_path) # Insert at the beginning to build the path in correct order\n","\n","    return best_path\n","\n","# Get a sample tweet for Viterbi decoding\n","sample_tweet_index = 0  # You can change this index to try different tweets\n","sample_tweet_text = df['cleaned_tweet_text'].iloc[sample_tweet_index]\n","sample_tweet_tokens = nltk.word_tokenize(sample_tweet_text)\n","\n","# Get all unique tags from the training data to use in Viterbi\n","# Convert set to list for consistent iteration order within the Viterbi function\n","all_tags = list(all_tags_set)\n","\n","print(f\"\\nApplying Viterbi decoding to sample tweet (index {sample_tweet_index}):\")\n","print(f\"Original Tweet: {sample_tweet_text}\")\n","print(f\"Tokenized: {sample_tweet_tokens}\")\n","\n","# Perform Viterbi decoding using the calculated HMM parameters\n","viterbi_path = viterbi_decode(sample_tweet_tokens, initial_probabilities, transition_probabilities, emission_probabilities, all_tags)\n","\n","print(f\"Viterbi Path (POS Tags): {viterbi_path}\")\n","\n","# Compare with actual POS tags (if available in the DataFrame)\n","if 'pos_tags' in df.columns:\n","    actual_pos_tags = [tag for word, tag in df['pos_tags'].iloc[sample_tweet_index]]\n","    print(f\"Actual POS Tags:   {actual_pos_tags}\")\n","\n","    # Calculate accuracy for this specific tweet\n","    correct_tags = sum(1 for p, a in zip(viterbi_path, actual_pos_tags) if p == a)\n","    accuracy = correct_tags / len(actual_pos_tags) if len(actual_pos_tags) > 0 else 0\n","    print(f\"Viterbi Accuracy for this tweet: {accuracy:.2f}\")\n","else:\n","    print(\"Actual POS tags not available for comparison.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kh0kftq2Rah9","executionInfo":{"status":"ok","timestamp":1769683057976,"user_tz":-330,"elapsed":12,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"1a97a55d-b4d1-4e87-9d77-193de16e6629"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Applying Viterbi decoding to sample tweet (index 0):\n","Original Tweet: when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n","Tokenized: ['when', 'modi', 'promised', '“', 'minimum', 'government', 'maximum', 'governance', '”', 'expected', 'him', 'begin', 'the', 'difficult', 'job', 'reforming', 'the', 'state', 'why', 'does', 'take', 'years', 'get', 'justice', 'state', 'should', 'and', 'not', 'business', 'and', 'should', 'exit', 'psus', 'and', 'temples']\n","Viterbi Path (POS Tags): ['NN', 'VBZ', 'VBN', 'NNP', 'NNP', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'PRP', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'NNS', 'VBP', 'RB', 'RB', 'RB', 'RB', 'RB', 'RB', 'CC', 'MD', 'NN', 'NNS', 'CC', 'NN']\n","Actual POS Tags:   ['WRB', 'NN', 'VBD', 'NNP', 'JJ', 'NN', 'JJ', 'NN', 'NNP', 'VBD', 'PRP', 'VB', 'DT', 'JJ', 'NN', 'VBG', 'DT', 'NN', 'WRB', 'VBZ', 'VB', 'NNS', 'VB', 'NN', 'NN', 'MD', 'CC', 'RB', 'NN', 'CC', 'MD', 'VB', 'NN', 'CC', 'NNS']\n","Viterbi Accuracy for this tweet: 0.40\n"]}]},{"cell_type":"markdown","source":["Hidden Markov Models (HMMs) can struggle significantly with social media text due to several inherent characteristics of this type of language:\n","\n","Informal and Unstructured Language: Social media posts are often highly informal, featuring slang, abbreviations (e.g., 'lol', 'brb'), misspellings, grammatical errors, and a general lack of adherence to standard linguistic rules. HMMs rely heavily on the statistical regularities found in more formal, structured language for their transition and emission probabilities. When these regularities are absent or significantly altered, the model's predictions become unreliable.\n","\n","Vocabulary Mismatch and Neologisms: Social media is a dynamic environment where new words, hashtags, and unique expressions (neologisms) emerge constantly. An HMM trained on traditional corpora will encounter many 'unknown words' in social media text. While techniques like Laplace smoothing can assign non-zero probabilities, these words will still have extremely low emission probabilities, making their POS tagging highly uncertain or often defaulting to a generic tag like a noun ('NN') regardless of their actual function (as we observed with some singletons in our analysis).\n","\n","Noise and Special Characters: Social media is replete with emojis, emoticons, multiple punctuation marks, URLs, mentions (@user), and hashtags (#topic). While we performed some cleaning (removing URLs and mentions), the remaining noise can still confuse the HMM's statistical patterns, as these elements don't fit neatly into standard POS categories.\n","\n","Ambiguity and Context Dependence: Many words in informal language can be highly ambiguous, and their meaning (and thus their POS) depends heavily on context. Social media posts are often short, providing limited context. HMMs, which primarily capture local dependencies (previous tag influencing current tag), may not be sophisticated enough to resolve these ambiguities accurately without richer contextual features.\n","\n","Domain Mismatch: Typically, HMMs used for POS tagging are trained on formal text datasets (like the Penn Treebank). Applying such a model directly to a vastly different domain like social media without retraining or adaptation leads to poor performance. The statistical distributions of tags and words within tags are simply different.\n","\n","Lack of Punctuation and Capitalization: The inconsistent use of punctuation and capitalization in social media can strip away cues that POS taggers, especially those relying on HMMs, often use to disambiguate words and identify sentence structure.\n","\n","In essence, the 'rules' (statistical patterns) an HMM learns from formal text don't transfer well to the 'wild west' of social media language, leading to frequent misclassifications and lower accuracy, as demonstrated by the 40% accuracy on our sample tweet."],"metadata":{"id":"sXzKE4lRR14F"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5H_86oX3B7F","executionInfo":{"status":"ok","timestamp":1769682866381,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"6951536d-327e-4316-8271-5992a3a51268"},"source":["import pandas as pd\n","import re\n","\n","# Load the dataset from the specified CSV file\n","try:\n","    df = pd.read_csv('/content/Twitter_Data.csv')\n","    print(\"Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: Twitter_Data.csv not found. Please ensure the file is in the /content/ directory.\")\n","    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors if file is not found\n","\n","if not df.empty:\n","    # Inspect available columns to identify the tweet text column\n","    print(\"\\nAvailable columns:\", df.columns.tolist())\n","\n","    # Define common column names that might contain tweet text\n","    text_column_candidates = ['clean_text', 'text', 'tweet', 'Tweet', 'content', 'message']\n","    tweet_text_column = None\n","\n","    # Iterate through candidates to find the actual tweet text column\n","    for col in text_column_candidates:\n","        if col in df.columns:\n","            tweet_text_column = col\n","            break\n","\n","    if tweet_text_column is None:\n","        print(\"\\nCould not find a common tweet text column. Please identify the correct column from the list above and update the 'tweet_text_column' variable.\")\n","    else:\n","        print(f\"\\nUsing '{tweet_text_column}' as the tweet text column.\")\n","\n","        # Define a function to remove URLs from text\n","        def remove_urls(text):\n","            \"\"\"\n","            Removes URLs from a given string.\n","\n","            Args:\n","                text (str): The input string.\n","\n","            Returns:\n","                str: The string with URLs removed.\n","            \"\"\"\n","            url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","            return url_pattern.sub(r'', text)\n","\n","        # Define a function to remove mentions (@username) from text\n","        def remove_mentions(text):\n","            \"\"\"\n","            Removes Twitter mentions (e.g., @username) from a given string.\n","\n","            Args:\n","                text (str): The input string.\n","\n","            Returns:\n","                str: The string with mentions removed.\n","            \"\"\"\n","            mention_pattern = re.compile(r'@\\w+')\n","            return mention_pattern.sub(r'', text)\n","\n","        # Apply preprocessing steps to create a new 'cleaned_tweet_text' column\n","        print(\"\\nPreprocessing tweet text...\")\n","        # Convert the column to string type to avoid errors with non-string entries\n","        df['cleaned_tweet_text'] = df[tweet_text_column].astype(str).apply(remove_urls)\n","        df['cleaned_tweet_text'] = df['cleaned_tweet_text'].apply(remove_mentions)\n","\n","        print(\"\\nOriginal tweet examples:\")\n","        print(df[tweet_text_column].head())\n","        print(\"\\nCleaned tweet examples:\")\n","        print(df['cleaned_tweet_text'].head())\n","        print(\"\\nPreprocessing complete. A new column 'cleaned_tweet_text' has been added to the DataFrame.\")\n","else:\n","    print(\"DataFrame is empty, unable to perform preprocessing.\")"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded successfully.\n","\n","Available columns: ['clean_text', 'category']\n","\n","Using 'clean_text' as the tweet text column.\n","\n","Preprocessing tweet text...\n","\n","Original tweet examples:\n","0    when modi promised “minimum government maximum...\n","1    talk all the nonsense and continue all the dra...\n","2    what did just say vote for modi  welcome bjp t...\n","3    asking his supporters prefix chowkidar their n...\n","4    answer who among these the most powerful world...\n","Name: clean_text, dtype: object\n","\n","Cleaned tweet examples:\n","0    when modi promised “minimum government maximum...\n","1    talk all the nonsense and continue all the dra...\n","2    what did just say vote for modi  welcome bjp t...\n","3    asking his supporters prefix chowkidar their n...\n","4    answer who among these the most powerful world...\n","Name: cleaned_tweet_text, dtype: object\n","\n","Preprocessing complete. A new column 'cleaned_tweet_text' has been added to the DataFrame.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":724},"id":"908d1f2e","executionInfo":{"status":"ok","timestamp":1769683048064,"user_tz":-330,"elapsed":181115,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"1b159782-c340-44f1-93b1-61c552671936"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","\n","# Download necessary NLTK data for POS tagging and lemmatization if not already present\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    print(\"Downloading 'punkt' for NLTK...\")\n","    nltk.download('punkt')\n","    print(\"'punkt' downloaded.\")\n","\n","try:\n","    # Corrected resource name to 'averaged_perceptron_tagger' for POS tagging\n","    nltk.data.find('taggers/averaged_perceptron_tagger')\n","except LookupError:\n","    print(\"Downloading 'averaged_perceptron_tagger' for NLTK...\")\n","    nltk.download('averaged_perceptron_tagger')\n","    print(\"'averaged_perceptron_tagger' downloaded.\")\n","\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    print(\"Downloading 'wordnet' for NLTK...\")\n","    nltk.download('wordnet')\n","    print(\"'wordnet' downloaded.\")\n","\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    print(\"Downloading 'stopwords' for NLTK...\")\n","    nltk.download('stopwords')\n","    print(\"'stopwords' downloaded.\")\n","\n","print(\"NLTK and its tagger data are ready.\")\n","\n","# Function to perform POS tagging on a given text\n","def pos_tag_text(text):\n","    \"\"\"\n","    Tokenizes the input text and performs Part-of-Speech tagging.\n","\n","    Args:\n","        text (str): The input string (e.g., a cleaned tweet).\n","\n","    Returns:\n","        list: A list of (word, tag) tuples.\n","    \"\"\"\n","    tokens = nltk.word_tokenize(text)\n","    return nltk.pos_tag(tokens)\n","\n","# Function to convert NLTK POS tags to WordNet POS tags, required for accurate lemmatization\n","def get_wordnet_pos(tag):\n","    \"\"\"\n","    Converts an NLTK POS tag to a WordNet POS tag.\n","    This is necessary for the WordNetLemmatizer to work correctly.\n","\n","    Args:\n","        tag (str): The NLTK POS tag.\n","\n","    Returns:\n","        str: The corresponding WordNet POS tag (e.g., ADJ, VERB, NOUN, ADV).\n","             Defaults to NOUN if no specific mapping is found.\n","    \"\"\"\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN # Default to noun if not found\n","\n","# Initialize the WordNet lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","# Get a set of English stopwords for efficient lookup\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to lemmatize tokens and remove stopwords\n","def lemmatize_and_remove_stopwords(pos_tags):\n","    \"\"\"\n","    Lemmatizes words based on their POS tags and removes common English stopwords.\n","\n","    Args:\n","        pos_tags (list): A list of (word, tag) tuples from NLTK's pos_tag function.\n","\n","    Returns:\n","        str: A space-separated string of lemmatized, non-stopwords.\n","    \"\"\"\n","    lemmatized_tokens = []\n","    for word, tag in pos_tags:\n","        # Convert to lowercase and remove non-alphabetic characters\n","        cleaned_word = ''.join(filter(str.isalpha, word)).lower()\n","        # Process only if the word is not empty and not a stopword\n","        if cleaned_word and cleaned_word not in stop_words:\n","            wntag = get_wordnet_pos(tag)\n","            lemmatized_tokens.append(lemmatizer.lemmatize(cleaned_word, wntag))\n","    return ' '.join(lemmatized_tokens)\n","\n","\n","if not df.empty and 'cleaned_tweet_text' in df.columns:\n","    print(\"\\nPerforming POS tagging on 'cleaned_tweet_text'...\")\n","    # Apply POS tagging to the cleaned tweet text and store in a new 'pos_tags' column\n","    df['pos_tags'] = df['cleaned_tweet_text'].astype(str).apply(pos_tag_text)\n","    print(\"POS tagging complete. A new column 'pos_tags' has been added to the DataFrame.\")\n","    print(\"\\nExamples of POS tags:\")\n","    display(df[['cleaned_tweet_text', 'pos_tags']].head())\n","\n","    print(\"\\nPerforming lemmatization and stopword removal...\")\n","    # Apply lemmatization and stopword removal using the 'pos_tags' column\n","    df['lemmatized_text'] = df['pos_tags'].apply(lemmatize_and_remove_stopwords)\n","    print(\"Lemmatization and stopword removal complete. A new column 'lemmatized_text' has been added.\")\n","    print(\"\\nExamples of lemmatized text:\")\n","    display(df[['cleaned_tweet_text', 'pos_tags', 'lemmatized_text']].head())\n","else:\n","    print(\"DataFrame is empty or 'cleaned_tweet_text' column not found. Cannot perform POS tagging, lemmatization or stopword removal.\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 'wordnet' for NLTK...\n","'wordnet' downloaded.\n","NLTK and its tagger data are ready.\n","\n","Performing POS tagging on 'cleaned_tweet_text'...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["POS tagging complete. A new column 'pos_tags' has been added to the DataFrame.\n","\n","Examples of POS tags:\n"]},{"output_type":"display_data","data":{"text/plain":["                                  cleaned_tweet_text  \\\n","0  when modi promised “minimum government maximum...   \n","1  talk all the nonsense and continue all the dra...   \n","2  what did just say vote for modi  welcome bjp t...   \n","3  asking his supporters prefix chowkidar their n...   \n","4  answer who among these the most powerful world...   \n","\n","                                            pos_tags  \n","0  [(when, WRB), (modi, NN), (promised, VBD), (“,...  \n","1  [(talk, NN), (all, PDT), (the, DT), (nonsense,...  \n","2  [(what, WP), (did, VBD), (just, RB), (say, VB)...  \n","3  [(asking, VBG), (his, PRP$), (supporters, NNS)...  \n","4  [(answer, NN), (who, WP), (among, IN), (these,...  "],"text/html":["\n","  <div id=\"df-3213795f-c000-4a9d-ab4e-2908a8044694\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_tweet_text</th>\n","      <th>pos_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>[(when, WRB), (modi, NN), (promised, VBD), (“,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>[(talk, NN), (all, PDT), (the, DT), (nonsense,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>[(what, WP), (did, VBD), (just, RB), (say, VB)...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>[(asking, VBG), (his, PRP$), (supporters, NNS)...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>[(answer, NN), (who, WP), (among, IN), (these,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3213795f-c000-4a9d-ab4e-2908a8044694')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3213795f-c000-4a9d-ab4e-2908a8044694 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3213795f-c000-4a9d-ab4e-2908a8044694');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"DataFrame is empty or 'cleaned_tweet_text' column not found\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk all the nonsense and continue all the drama will vote for modi \",\n          \"answer who among these the most powerful world leader today trump putin modi may \",\n          \"what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Performing lemmatization and stopword removal...\n","Lemmatization and stopword removal complete. A new column 'lemmatized_text' has been added.\n","\n","Examples of lemmatized text:\n"]},{"output_type":"display_data","data":{"text/plain":["                                  cleaned_tweet_text  \\\n","0  when modi promised “minimum government maximum...   \n","1  talk all the nonsense and continue all the dra...   \n","2  what did just say vote for modi  welcome bjp t...   \n","3  asking his supporters prefix chowkidar their n...   \n","4  answer who among these the most powerful world...   \n","\n","                                            pos_tags  \\\n","0  [(when, WRB), (modi, NN), (promised, VBD), (“,...   \n","1  [(talk, NN), (all, PDT), (the, DT), (nonsense,...   \n","2  [(what, WP), (did, VBD), (just, RB), (say, VB)...   \n","3  [(asking, VBG), (his, PRP$), (supporters, NNS)...   \n","4  [(answer, NN), (who, WP), (among, IN), (these,...   \n","\n","                                     lemmatized_text  \n","0  modi promise minimum government maximum govern...  \n","1             talk nonsense continue drama vote modi  \n","2  say vote modi welcome bjp tell rahul main camp...  \n","3  ask supporter prefix chowkidar name modi great...  \n","4  answer among powerful world leader today trump...  "],"text/html":["\n","  <div id=\"df-bbd17bc0-c72c-45ec-a285-fb31dbde7ca4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_tweet_text</th>\n","      <th>pos_tags</th>\n","      <th>lemmatized_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>[(when, WRB), (modi, NN), (promised, VBD), (“,...</td>\n","      <td>modi promise minimum government maximum govern...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>[(talk, NN), (all, PDT), (the, DT), (nonsense,...</td>\n","      <td>talk nonsense continue drama vote modi</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>[(what, WP), (did, VBD), (just, RB), (say, VB)...</td>\n","      <td>say vote modi welcome bjp tell rahul main camp...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>[(asking, VBG), (his, PRP$), (supporters, NNS)...</td>\n","      <td>ask supporter prefix chowkidar name modi great...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>[(answer, NN), (who, WP), (among, IN), (these,...</td>\n","      <td>answer among powerful world leader today trump...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbd17bc0-c72c-45ec-a285-fb31dbde7ca4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bbd17bc0-c72c-45ec-a285-fb31dbde7ca4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bbd17bc0-c72c-45ec-a285-fb31dbde7ca4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"DataFrame is empty or 'cleaned_tweet_text' column not found\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk all the nonsense and continue all the drama will vote for modi \",\n          \"answer who among these the most powerful world leader today trump putin modi may \",\n          \"what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"talk nonsense continue drama vote modi\",\n          \"answer among powerful world leader today trump putin modi may\",\n          \"say vote modi welcome bjp tell rahul main campaigner modi think modi relax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"893c528f","executionInfo":{"status":"ok","timestamp":1769683053860,"user_tz":-330,"elapsed":4911,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"d3a83e15-81fa-4ed9-c102-8334a5e84665"},"source":["from collections import defaultdict\n","\n","if not df.empty and 'pos_tags' in df.columns:\n","    print(\"\\nBuilding HMM parameters...\")\n","\n","    # Initialize dictionaries for HMM parameters with Laplace smoothing (add 1)\n","    # Laplace smoothing ensures no zero probabilities, which can cause issues with logarithms.\n","    initial_probabilities = defaultdict(lambda: 1)\n","    transition_probabilities = defaultdict(lambda: defaultdict(lambda: 1))\n","    emission_probabilities = defaultdict(lambda: defaultdict(lambda: 1))\n","    tag_counts = defaultdict(lambda: 0) # Used to normalize probabilities later\n","\n","    all_tags_set = set() # To collect all unique tags encountered in the dataset\n","\n","    # Process each tweet's POS tags to build up counts for HMM parameters\n","    for tags_sequence in df['pos_tags']:\n","        if tags_sequence:\n","            # Initial probabilities: increment count for the first tag in each sequence\n","            first_tag = tags_sequence[0][1] # (word, tag) -> tag\n","            initial_probabilities[first_tag] += 1\n","            all_tags_set.add(first_tag)\n","\n","            # Transition and Emission probabilities: iterate through each word-tag pair\n","            for i, (word, tag) in enumerate(tags_sequence):\n","                tag_counts[tag] += 1 # Count occurrences of each tag\n","                all_tags_set.add(tag)\n","                # Emission probabilities: count how many times a word is 'emitted' by a tag\n","                emission_probabilities[tag][word.lower()] += 1 # Use lowercase for words for consistency\n","\n","                # Transition probabilities: count transitions from previous tag to current tag\n","                if i > 0:\n","                    prev_tag = tags_sequence[i-1][1]\n","                    transition_probabilities[prev_tag][tag] += 1\n","\n","    # Convert tag_counts to a regular dict for easier access/iteration (though defaultdict works too)\n","    tag_counts_dict = dict(tag_counts)\n","\n","    # Normalize probabilities for Initial, Transition, and Emission parameters\n","    # Normalize Initial probabilities\n","    total_initial_count = sum(initial_probabilities.values())\n","    initial_probabilities = {tag: count / total_initial_count for tag, count in initial_probabilities.items()}\n","\n","    # Normalize Transition probabilities\n","    for prev_tag, next_tag_counts in transition_probabilities.items():\n","        total_transitions_from_prev_tag = sum(next_tag_counts.values())\n","        # Ensure we don't divide by zero if a tag never transitions to anything else\n","        if total_transitions_from_prev_tag > 0:\n","            transition_probabilities[prev_tag] = {next_tag: count / total_transitions_from_prev_tag for next_tag, count in next_tag_counts.items()}\n","        else:\n","            # If a tag never transitions, its probabilities remain as the smoothed default (1/N, where N is num of possible next states)\n","            transition_probabilities[prev_tag] = {next_tag: 1/len(all_tags_set) for next_tag in all_tags_set} # Fallback smoothing\n","\n","    # Normalize Emission probabilities\n","    for tag, word_counts in emission_probabilities.items():\n","        total_words_for_tag = sum(word_counts.values())\n","        # Ensure we don't divide by zero if a tag never emits a word (unlikely with smoothing but good practice)\n","        if total_words_for_tag > 0:\n","            emission_probabilities[tag] = {word: count / total_words_for_tag for word, count in word_counts.items()}\n","        else:\n","            # Fallback smoothing for emission if a tag has no observed words (highly unlikely)\n","            emission_probabilities[tag] = {word: 1/len(word_frequencies) for word in word_frequencies} # Use total unique words\n","\n","    print(\"HMM parameters built successfully.\")\n","    print(\"\\n--- HMM Parameters Examples ---\")\n","    print(\"\\nInitial Probabilities (top 5):\")\n","    print(dict(list(initial_probabilities.items())[:5]))\n","    print(\"\\nTransition Probabilities (first 3 tags, top 3 transitions each):\")\n","    for i, (prev_tag, next_tag_probs) in enumerate(transition_probabilities.items()):\n","        if i >= 3: break\n","        print(f\"  {prev_tag}: {dict(list(next_tag_probs.items())[:3])}\")\n","    print(\"\\nEmission Probabilities (first 3 tags, top 3 emissions each):\")\n","    for i, (tag, word_probs) in enumerate(emission_probabilities.items()):\n","        if i >= 3: break\n","        print(f\"  {tag}: {dict(list(word_probs.items())[:3])}\")\n","\n","else:\n","    print(\"DataFrame is empty or 'pos_tags' column not found. Cannot build HMM parameters.\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Building HMM parameters...\n","HMM parameters built successfully.\n","\n","--- HMM Parameters Examples ---\n","\n","Initial Probabilities (top 5):\n","{'WRB': 0.030844922672981247, 'NN': 0.3353557165555276, 'WP': 0.017520505002791256, 'VBG': 0.019741241281156254, 'JJ': 0.13076578593819974}\n","\n","Transition Probabilities (first 3 tags, top 3 transitions each):\n","  WRB: {'NN': 0.26288186705126526, 'VBZ': 0.028648223416808706, 'JJS': 0.0008522354792185657}\n","  NN: {'VBD': 0.06080158149560789, 'JJ': 0.054530325960094043, 'NNP': 0.008668070206789487}\n","  VBD: {'NNP': 0.0049813780260707635, 'PRP': 0.03550279329608939, 'RB': 0.09728119180633148}\n","\n","Emission Probabilities (first 3 tags, top 3 emissions each):\n","  WRB: {'when': 0.2616143264988321, 'why': 0.351901116013496, 'how': 0.2508434985725409}\n","  NN: {'modi': 0.09749772590027594, 'government': 0.005617408520038831, 'governance': 0.0004959066204967092}\n","  VBD: {'promised': 0.006117570450592479, 'expected': 0.000676874041452109, 'did': 0.06012183732746138}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b6a9531","executionInfo":{"status":"ok","timestamp":1769683055000,"user_tz":-330,"elapsed":14,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"e22ccbf4-9d89-4596-8279-9dd07de29dcd"},"source":["import math\n","\n","if not df.empty and 'pos_tags' in df.columns and transition_probabilities:\n","    print(\"\\n--- Analyzing Transition Probability Irregularities ---\")\n","\n","    def print_top_transitions(prev_tag, num=5):\n","        \"\"\"\n","        Prints the top N most probable next tags for a given previous tag.\n","\n","        Args:\n","            prev_tag (str): The POS tag for which to show transitions.\n","            num (int): The number of top transitions to display.\n","        \"\"\"\n","        if prev_tag in transition_probabilities:\n","            # Sort transitions by probability in descending order\n","            sorted_transitions = sorted(transition_probabilities[prev_tag].items(), key=lambda item: item[1], reverse=True)\n","            print(f\"  Top {num} transitions from '{prev_tag}':\")\n","            for next_tag, prob in sorted_transitions[:num]:\n","                print(f\"    -> {next_tag}: {prob:.4f}\")\n","        else:\n","            print(f\"  No transitions found for '{prev_tag}'.\")\n","\n","    print(\"\\nExamples of Transition Probabilities for common tags:\")\n","    print_top_transitions('NN') # Noun\n","    print_top_transitions('VB') # Verb, base form\n","    print_top_transitions('JJ') # Adjective\n","    print_top_transitions('DT') # Determiner\n","    print_top_transitions('IN') # Preposition/subordinating conjunction\n","\n","    # Calculate entropy for transition probabilities for each previous tag\n","    # Entropy quantifies the predictability; higher entropy means less predictable next tags.\n","    # Formula: Entropy = - sum(p * log(p))\n","    transition_entropies = {}\n","    for prev_tag, next_tag_probs in transition_probabilities.items():\n","        entropy = 0\n","        for prob in next_tag_probs.values():\n","            if prob > 0: # Avoid log(0) which is undefined\n","                entropy -= prob * math.log(prob, 2) # Using log base 2 for bits\n","        transition_entropies[prev_tag] = entropy\n","\n","    # Sort tags by entropy to find those with most/least predictable transitions\n","    sorted_entropies = sorted(transition_entropies.items(), key=lambda item: item[1])\n","\n","    print(\"\\nTags with the lowest transition entropy (most predictable next tags):\")\n","    for tag, entropy in sorted_entropies[:5]: # Display top 5 lowest entropy tags\n","        print(f\"  '{tag}': {entropy:.4f} bits\")\n","        print_top_transitions(tag, num=3) # Show top 3 transitions for context\n","\n","    print(\"\\nTags with the highest transition entropy (least predictable next tags):\")\n","    for tag, entropy in sorted_entropies[-5:]: # Display top 5 highest entropy tags\n","        print(f\"  '{tag}': {entropy:.4f} bits\")\n","        print_top_transitions(tag, num=3) # Show top 3 transitions for context\n","\n","else:\n","    print(\"HMM parameters (transition_probabilities) not available for analysis.\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Analyzing Transition Probability Irregularities ---\n","\n","Examples of Transition Probabilities for common tags:\n","  Top 5 transitions from 'NN':\n","    -> NN: 0.3595\n","    -> IN: 0.0932\n","    -> VBD: 0.0608\n","    -> NNS: 0.0592\n","    -> JJ: 0.0545\n","  Top 5 transitions from 'VB':\n","    -> NN: 0.1905\n","    -> DT: 0.1486\n","    -> JJ: 0.1187\n","    -> PRP$: 0.0864\n","    -> IN: 0.0790\n","  Top 5 transitions from 'JJ':\n","    -> NN: 0.5665\n","    -> NNS: 0.1530\n","    -> JJ: 0.0995\n","    -> IN: 0.0329\n","    -> RB: 0.0208\n","  Top 5 transitions from 'DT':\n","    -> NN: 0.5083\n","    -> JJ: 0.1983\n","    -> NNS: 0.1178\n","    -> JJS: 0.0183\n","    -> RB: 0.0168\n","  Top 5 transitions from 'IN':\n","    -> NN: 0.3217\n","    -> JJ: 0.1733\n","    -> DT: 0.1480\n","    -> NNS: 0.0720\n","    -> PRP: 0.0532\n","\n","Tags with the lowest transition entropy (most predictable next tags):\n","  'SYM': 0.0000 bits\n","  Top 3 transitions from 'SYM':\n","    -> NN: 1.0000\n","  '``': 0.0000 bits\n","  Top 3 transitions from '``':\n","    -> RB: 1.0000\n","  '$': 0.0725 bits\n","  Top 3 transitions from '$':\n","    -> CD: 0.9912\n","    -> RB: 0.0088\n","  'PDT': 0.3935 bits\n","  Top 3 transitions from 'PDT':\n","    -> DT: 0.9409\n","    -> PRP$: 0.0476\n","    -> IN: 0.0059\n","  'POS': 1.2136 bits\n","  Top 3 transitions from 'POS':\n","    -> NN: 0.7750\n","    -> WRB: 0.0750\n","    -> CD: 0.0500\n","\n","Tags with the highest transition entropy (least predictable next tags):\n","  'CC': 3.7163 bits\n","  Top 3 transitions from 'CC':\n","    -> NN: 0.2375\n","    -> JJ: 0.1443\n","    -> VB: 0.0962\n","  'VBD': 3.7220 bits\n","  Top 3 transitions from 'VBD':\n","    -> JJ: 0.1855\n","    -> NN: 0.1557\n","    -> DT: 0.1156\n","  'VBP': 3.7570 bits\n","  Top 3 transitions from 'VBP':\n","    -> JJ: 0.2008\n","    -> NN: 0.1456\n","    -> DT: 0.0897\n","  'VB': 3.7888 bits\n","  Top 3 transitions from 'VB':\n","    -> NN: 0.1905\n","    -> DT: 0.1486\n","    -> JJ: 0.1187\n","  'RB': 3.9364 bits\n","  Top 3 transitions from 'RB':\n","    -> JJ: 0.1841\n","    -> VB: 0.1214\n","    -> RB: 0.0913\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-0C7luzQ1H","executionInfo":{"status":"ok","timestamp":1769683057347,"user_tz":-330,"elapsed":1428,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"69450f90-ac9a-4437-91fa-c054caeae5df"},"source":["from collections import defaultdict\n","\n","print(\"\\n--- Analyzing Rare and Low-Probability Tokens ---\")\n","\n","# Identify words with the lowest emission probabilities for each tag\n","print(\"\\nTop 5 words with the lowest emission probabilities for each of the first 5 tags:\")\n","# Iterate through a few tags to show examples\n","for i, (tag, word_probs) in enumerate(emission_probabilities.items()):\n","    if i >= 5: break # Limit to the first 5 tags for brevity\n","    if word_probs:\n","        # Sort words by their emission probability in ascending order\n","        sorted_words = sorted(word_probs.items(), key=lambda item: item[1])\n","        print(f\"  Tag '{tag}':\")\n","        for word, prob in sorted_words[:5]: # Display top 5 lowest probability words\n","            print(f\"    '{word}': {prob:.8f}\")\n","    else:\n","        print(f\"  Tag '{tag}': No emission probabilities found.\")\n","\n","# Calculate overall word frequencies to find truly rare words in the dataset\n","word_frequencies = defaultdict(int)\n","for tags_sequence in df['pos_tags']:\n","    if tags_sequence:\n","        for word, _ in tags_sequence:\n","            word_frequencies[word.lower()] += 1 # Count all word occurrences, lowercased\n","\n","# Filter for words that appeared only once (singletons) in the entire dataset\n","singletons = {word: count for word, count in word_frequencies.items() if count == 1}\n","\n","print(\"\\nExamples of words that appeared only once in the entire dataset (singletons):\")\n","if singletons:\n","    # Display first 10 singletons for example\n","    for i, (word, count) in enumerate(list(singletons.items())[:10]):\n","        print(f\"  '{word}' (count: {count})\")\n","        # Attempt to find and print their emission probability if available\n","        found_tags_for_singleton = []\n","        for tags_sequence_full in df['pos_tags']:\n","            if tags_sequence_full:\n","                for w, t in tags_sequence_full:\n","                    if w.lower() == word:\n","                        found_tags_for_singleton.append(t)\n","                        break # Only need one tag for example, assuming consistent tagging\n","                if found_tags_for_singleton: break # Stop after finding the first tag\n","\n","        if found_tags_for_singleton and found_tags_for_singleton[0] in emission_probabilities and word in emission_probabilities[found_tags_for_singleton[0]]:\n","             print(f\"    Emission probability for tag '{found_tags_for_singleton[0]}': {emission_probabilities[found_tags_for_singleton[0]][word]:.8f}\")\n","        else:\n","            # Due to Laplace smoothing, a word will always have a small probability,\n","            # but if the tag itself wasn't in emission_probabilities or the word wasn't\n","            # directly in its specific smoothed entry, it might be a general default.\n","            print(f\"    Emission probability for tag not found or word not in emission for its tag (due to smoothing, this might be a small default value).\")\n","else:\n","    print(\"  No singletons found.\")\n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Analyzing Rare and Low-Probability Tokens ---\n","\n","Top 5 words with the lowest emission probabilities for each of the first 5 tags:\n","  Tag 'WRB':\n","    'mover': 0.00006488\n","    'wasn': 0.00006488\n","    'wld': 0.00006488\n","    'wiselythe': 0.00006488\n","    'whatsapp': 0.00006488\n","  Tag 'NN':\n","    'constituency2': 0.00000191\n","    'tuthukudi': 0.00000191\n","    'thuthukudi': 0.00000191\n","    'leadershipwho': 0.00000191\n","    'modiganga': 0.00000191\n","  Tag 'VBD':\n","    'disabilityif': 0.00001714\n","    'bursted': 0.00001714\n","    'mki': 0.00001714\n","    'stemmed': 0.00001714\n","    'piggybacked': 0.00001714\n","  Tag 'NNP':\n","    '➡': 0.00010847\n","    'िु': 0.00010847\n","    '⚡jai': 0.00010847\n","    'zayed': 0.00010847\n","    'mistakemodi': 0.00010847\n","  Tag 'JJ':\n","    'crustal': 0.00000494\n","    'maarkefir': 0.00000494\n","    'behaved': 0.00000494\n","    'likly': 0.00000494\n","    'insensitivearrogant': 0.00000494\n","\n","Examples of words that appeared only once in the entire dataset (singletons):\n","  'crustal' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'maarkefir' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'tax…the' (count: 1)\n","    Emission probability for tag 'VBP': 0.00001209\n","  'constituency2' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'tuthukudi' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'likly' (count: 1)\n","    Emission probability for tag 'JJ': 0.00000494\n","  'thuthukudi' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'leadershipwho' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n","  'thiugh' (count: 1)\n","    Emission probability for tag 'IN': 0.00000938\n","  'modiganga' (count: 1)\n","    Emission probability for tag 'NN': 0.00000191\n"]}]}]}