{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LjTfmVyjw94mem3V6SqtcKP4iKz5XwwE","timestamp":1770286623563}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9609d310","executionInfo":{"status":"ok","timestamp":1770286463598,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"739627a4-c45d-49ef-b63d-f2fb875b74bc"},"source":["import nltk # Natural Language Toolkit for working with human language data.\n","import numpy as np # Numerical Python, essential for numerical operations and array manipulation.\n","import pandas as pd # Data analysis and manipulation library, widely used for dataframes.\n","import re # Regular expression module, useful for text cleaning and pattern matching.\n","import math # Provides access to mathematical functions.\n","from collections import Counter, defaultdict # Counter for efficient frequency counting, defaultdict for dictionaries with default values.\n","import matplotlib.pyplot as plt # Matplotlib's pyplot module for creating static, animated, and interactive visualizations in Python.\n","\n","print(\"Libraries imported successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries imported successfully.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3442df44","executionInfo":{"status":"ok","timestamp":1770286463653,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"51b8e09d-718e-44ab-a18b-38e1df62ba25"},"source":["import nltk\n","nltk.download('gutenberg', quiet=True)\n","print(\"'gutenberg' corpus downloaded successfully (if not already present).\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'gutenberg' corpus downloaded successfully (if not already present).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b81e31f","executionInfo":{"status":"ok","timestamp":1770286464447,"user_tz":-330,"elapsed":793,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"03accdff-6485-49e3-b517-a60e1f489040"},"source":["text_raw = nltk.corpus.gutenberg.raw('austen-sense.txt')\n","text_lower = text_raw.lower()\n","\n","# Count words to ensure it meets the minimum requirement\n","words = nltk.word_tokenize(text_lower)\n","word_count = len(words)\n","\n","print(f\"Loaded text from 'austen-sense.txt'.\")\n","print(f\"Total words (after tokenization and lowercasing): {word_count}\")\n","\n","if word_count >= 1500:\n","    print(\"Text meets the minimum 1500 word requirement.\")\n","else:\n","    print(\"WARNING: Text does NOT meet the minimum 1500 word requirement. Please choose a larger text.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded text from 'austen-sense.txt'.\n","Total words (after tokenization and lowercasing): 141439\n","Text meets the minimum 1500 word requirement.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70cc3374","executionInfo":{"status":"ok","timestamp":1770286465480,"user_tz":-330,"elapsed":1025,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"ebd3e717-68cb-444b-a31d-1d1f28aad53f"},"source":["import nltk\n","nltk.download('punkt', quiet=True)\n","print(\"'punkt' tokenizer downloaded successfully (if not already present).\")\n","\n","text_raw = nltk.corpus.gutenberg.raw('austen-sense.txt')\n","text_lower = text_raw.lower()\n","\n","# Count words to ensure it meets the minimum requirement\n","words = nltk.word_tokenize(text_lower)\n","word_count = len(words)\n","\n","print(f\"Loaded text from 'austen-sense.txt'.\")\n","print(f\"Total words (after tokenization and lowercasing): {word_count}\")\n","\n","if word_count >= 1500:\n","    print(\"Text meets the minimum 1500 word requirement.\")\n","else:\n","    print(\"WARNING: Text does NOT meet the minimum 1500 word requirement. Please choose a larger text.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'punkt' tokenizer downloaded successfully (if not already present).\n","Loaded text from 'austen-sense.txt'.\n","Total words (after tokenization and lowercasing): 141439\n","Text meets the minimum 1500 word requirement.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edd5e417","executionInfo":{"status":"ok","timestamp":1770286466469,"user_tz":-330,"elapsed":989,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"4f92a155-4bcf-4f5e-a016-d79537fb343c"},"source":["import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True) # Attempt to download 'punkt_tab' as suggested by error message\n","print(\"'punkt' and 'punkt_tab' resources downloaded successfully (if not already present).\")\n","\n","text_raw = nltk.corpus.gutenberg.raw('austen-sense.txt')\n","text_lower = text_raw.lower()\n","\n","# Count words to ensure it meets the minimum requirement\n","words = nltk.word_tokenize(text_lower)\n","word_count = len(words)\n","\n","print(f\"Loaded text from 'austen-sense.txt'.\")\n","print(f\"Total words (after tokenization and lowercasing): {word_count}\")\n","\n","if word_count >= 1500:\n","    print(\"Text meets the minimum 1500 word requirement.\")\n","else:\n","    print(\"WARNING: Text does NOT meet the minimum 1500 word requirement. Please choose a larger text.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'punkt' and 'punkt_tab' resources downloaded successfully (if not already present).\n","Loaded text from 'austen-sense.txt'.\n","Total words (after tokenization and lowercasing): 141439\n","Text meets the minimum 1500 word requirement.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"400bfaa8","executionInfo":{"status":"ok","timestamp":1770286466635,"user_tz":-330,"elapsed":113,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"5dc2074b-e3ff-4aba-fa9c-536806a971dc"},"source":["sentences = nltk.sent_tokenize(text_lower)\n","print(f\"Total sentences tokenized: {len(sentences)}\")\n","print(\"First 3 sentences:\")\n","for i, sent in enumerate(sentences[:3]):\n","    print(f\"  {i+1}. {sent}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences tokenized: 4832\n","First 3 sentences:\n","  1. [sense and sensibility by jane austen 1811]\n","\n","chapter 1\n","\n","\n","the family of dashwood had long been settled in sussex.\n","  2. their estate was large, and their residence was at norland park,\n","in the centre of their property, where, for many generations,\n","they had lived in so respectable a manner as to engage\n","the general good opinion of their surrounding acquaintance.\n","  3. the late owner of this estate was a single man, who lived\n","to a very advanced age, and who for many years of his life,\n","had a constant companion and housekeeper in his sister.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da1190cf","executionInfo":{"status":"ok","timestamp":1770286467139,"user_tz":-330,"elapsed":505,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"a987dd1e-90f3-411d-db45-27365176b5ee"},"source":["processed_sentences = []\n","\n","for sentence in sentences:\n","    # Remove punctuation and numbers, and replace multiple spaces with single space\n","    cleaned_sentence = re.sub(r'[^a-z\\s]', '', sentence)\n","    cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n","\n","    # Tokenize the cleaned sentence into words\n","    words = nltk.word_tokenize(cleaned_sentence)\n","\n","    # Add start and end of sentence tokens if the sentence is not empty\n","    if words:\n","        processed_sentences.append(['<s>'] + words + ['</s>'])\n","\n","# Flatten processed_sentences into a single list of all processed words\n","all_processed_words = [word for sent in processed_sentences for word in sent]\n","\n","print(f\"Number of processed sentences: {len(processed_sentences)}\")\n","print(f\"Total words in all_processed_words (including <s>/</s>): {len(all_processed_words)}\")\n","print(\"First 3 processed sentences (tokenized with <s>/</s>):\")\n","for i, psent in enumerate(processed_sentences[:3]):\n","    print(f\"  {i+1}. {psent}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of processed sentences: 4832\n","Total words in all_processed_words (including <s>/</s>): 128389\n","First 3 processed sentences (tokenized with <s>/</s>):\n","  1. ['<s>', 'sense', 'and', 'sensibility', 'by', 'jane', 'austen', 'chapter', 'the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex', '</s>']\n","  2. ['<s>', 'their', 'estate', 'was', 'large', 'and', 'their', 'residence', 'was', 'at', 'norland', 'park', 'in', 'the', 'centre', 'of', 'their', 'property', 'where', 'for', 'many', 'generations', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '</s>']\n","  3. ['<s>', 'the', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', 'had', 'a', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', '</s>']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67ebd1a4","executionInfo":{"status":"ok","timestamp":1770286467193,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"64f6190a-6d18-4775-d5a3-bb22a09305eb"},"source":["from collections import Counter, defaultdict\n","\n","# 1. Initialize defaultdict objects\n","unigram_counts = defaultdict(int)\n","# For bigrams, we need to count (word_i, word_{i+1}) and also word_i for the denominator\n","bigram_counts = defaultdict(lambda: defaultdict(int))\n","bigram_prefix_counts = defaultdict(int)\n","\n","# For trigrams, we need to count (word_i, word_{i+1}, word_{i+2}) and also (word_i, word_{i+1}) for the denominator\n","trigram_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n","trigram_prefix_counts = defaultdict(lambda: defaultdict(int))\n","\n","print(\"Frequency counting dictionaries initialized.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Frequency counting dictionaries initialized.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b44b6646","executionInfo":{"status":"ok","timestamp":1770286467703,"user_tz":-330,"elapsed":468,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"235cf479-6446-4e67-a3a6-fada48f5daa5"},"source":["for sentence_tokens in processed_sentences:\n","    # Unigram counts\n","    for word in sentence_tokens:\n","        unigram_counts[word] += 1\n","\n","    # Bigram counts and prefix counts\n","    for i in range(len(sentence_tokens) - 1):\n","        prev_word = sentence_tokens[i]\n","        current_word = sentence_tokens[i+1]\n","        bigram_counts[prev_word][current_word] += 1\n","        bigram_prefix_counts[prev_word] += 1 # Denominator for bigram probability\n","\n","    # Trigram counts and prefix counts\n","    for i in range(len(sentence_tokens) - 2):\n","        prev_word1 = sentence_tokens[i]\n","        prev_word2 = sentence_tokens[i+1]\n","        current_word = sentence_tokens[i+2]\n","        trigram_counts[prev_word1][prev_word2][current_word] += 1\n","        trigram_prefix_counts[prev_word1][prev_word2] += 1 # Denominator for trigram probability\n","\n","# Calculate total number of unigrams (total words including start/end tokens)\n","total_unigrams = sum(unigram_counts.values())\n","\n","print(f\"Total unigrams (words including <s>/</s>): {total_unigrams}\")\n","print(\"N-gram frequency counts populated.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total unigrams (words including <s>/</s>): 128389\n","N-gram frequency counts populated.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4f295e2","executionInfo":{"status":"ok","timestamp":1770286467706,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"9e4d6b7d-bb26-4721-9f4f-e55e84c68d9e"},"source":["def get_unigram_prob(word):\n","    # Add a small epsilon to avoid division by zero for unseen words if total_unigrams is 0\n","    # Although total_unigrams should always be > 0 given our data\n","    if total_unigrams == 0:\n","        return 0.0 # Or some other appropriate default\n","    return unigram_counts[word] / total_unigrams\n","\n","def get_bigram_prob(prev_word, current_word):\n","    # P(current_word | prev_word) = Count(prev_word, current_word) / Count(prev_word)\n","    if bigram_prefix_counts[prev_word] == 0:\n","        return 0.0 # This bigram prefix (prev_word) was never seen\n","    return bigram_counts[prev_word][current_word] / bigram_prefix_counts[prev_word]\n","\n","def get_trigram_prob(prev_word1, prev_word2, current_word):\n","    # P(current_word | prev_word1, prev_word2) = Count(prev_word1, prev_word2, current_word) / Count(prev_word1, prev_word2)\n","    if trigram_prefix_counts[prev_word1][prev_word2] == 0:\n","        return 0.0 # This trigram prefix (prev_word1, prev_word2) was never seen\n","    return trigram_counts[prev_word1][prev_word2][current_word] / trigram_prefix_counts[prev_word1][prev_word2]\n","\n","print(\"Probability calculation functions defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability calculation functions defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41e8720b","executionInfo":{"status":"ok","timestamp":1770286467709,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"05ab9849-ee47-4992-8531-25aad28ae488"},"source":["print(\"\\n--- Example Frequencies and Probabilities ---\")\n","\n","# Unigram Examples\n","print(\"\\nUnigram Examples:\")\n","# Get top 5 most common unigrams\n","common_unigrams = Counter(unigram_counts).most_common(5)\n","for word, count in common_unigrams:\n","    prob = get_unigram_prob(word)\n","    print(f\"  Word: '{word}', Count: {count}, Probability: {prob:.6f}\")\n","\n","# Bigram Examples\n","print(\"\\nBigram Examples:\")\n","# Examples: bigrams starting with '<s>' and a common word like 'the'\n","\n","# Bigrams starting with '<s>'\n","start_bigrams = sorted([(w, bigram_counts['<s>'][w], get_bigram_prob('<s>', w)) for w in bigram_counts['<s>']], key=lambda x: x[1], reverse=True)[:5]\n","print(\"  Top 5 Bigrams starting with '<s>':\")\n","for word, count, prob in start_bigrams:\n","    print(f\"    ('<s>', '{word}'), Count: {count}, Probability: {prob:.6f}\")\n","\n","# Bigrams starting with 'the'\n","the_bigrams = sorted([(w, bigram_counts['the'][w], get_bigram_prob('the', w)) for w in bigram_counts['the']], key=lambda x: x[1], reverse=True)[:5]\n","print(\"  Top 5 Bigrams starting with 'the':\")\n","for word, count, prob in the_bigrams:\n","    print(f\"    ('the', '{word}'), Count: {count}, Probability: {prob:.6f}\")\n","\n","# Trigram Examples\n","print(\"\\nTrigram Examples:\")\n","# Examples: trigrams starting with '<s> <s>' (if applicable) and 'i am'\n","\n","# Find a common bigram prefix to demonstrate trigrams, e.g., ('<s>', 'the')\n","if '<s>' in trigram_prefix_counts and 'the' in trigram_prefix_counts['<s>']:\n","    s_the_trigrams = sorted([(w, trigram_counts['<s>']['the'][w], get_trigram_prob('<s>', 'the', w))\n","                             for w in trigram_counts['<s>']['the']],\n","                            key=lambda x: x[1], reverse=True)[:5]\n","    if s_the_trigrams:\n","        print(\"  Top 5 Trigrams starting with ('<s>', 'the'):\")\n","        for word, count, prob in s_the_trigrams:\n","            print(f\"    ('<s>', 'the', '{word}'), Count: {count}, Probability: {prob:.6f}\")\n","    else:\n","        print(\"  No trigrams found starting with ('<s>', 'the').\")\n","else:\n","    print(\"  Bigram prefix ('<s>', 'the') not found for trigram examples.\")\n","\n","# Another common trigram prefix, e.g. 'of the'\n","if 'of' in trigram_prefix_counts and 'the' in trigram_prefix_counts['of']:\n","    of_the_trigrams = sorted([(w, trigram_counts['of']['the'][w], get_trigram_prob('of', 'the', w))\n","                              for w in trigram_counts['of']['the']],\n","                             key=lambda x: x[1], reverse=True)[:5]\n","    if of_the_trigrams:\n","        print(\"  Top 5 Trigrams starting with ('of', 'the'):\")\n","        for word, count, prob in of_the_trigrams:\n","            print(f\"    ('of', 'the', '{word}'), Count: {count}, Probability: {prob:.6f}\")\n","    else:\n","        print(\"  No trigrams found starting with ('of', 'the').\")\n","else:\n","    print(\"  Bigram prefix ('of', 'the') not found for trigram examples.\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Example Frequencies and Probabilities ---\n","\n","Unigram Examples:\n","  Word: '<s>', Count: 4832, Probability: 0.037636\n","  Word: '</s>', Count: 4832, Probability: 0.037636\n","  Word: 'the', Count: 4087, Probability: 0.031833\n","  Word: 'to', Count: 4086, Probability: 0.031825\n","  Word: 'of', Count: 3569, Probability: 0.027798\n","\n","Bigram Examples:\n","  Top 5 Bigrams starting with '<s>':\n","    ('<s>', 'i'), Count: 486, Probability: 0.100579\n","    ('<s>', 'but'), Count: 273, Probability: 0.056498\n","    ('<s>', 'she'), Count: 247, Probability: 0.051118\n","    ('<s>', 'the'), Count: 227, Probability: 0.046978\n","    ('<s>', 'he'), Count: 204, Probability: 0.042219\n","  Top 5 Bigrams starting with 'the':\n","    ('the', 'same'), Count: 99, Probability: 0.024223\n","    ('the', 'world'), Count: 87, Probability: 0.021287\n","    ('the', 'house'), Count: 85, Probability: 0.020798\n","    ('the', 'most'), Count: 63, Probability: 0.015415\n","    ('the', 'room'), Count: 62, Probability: 0.015170\n","\n","Trigram Examples:\n","  Top 5 Trigrams starting with ('<s>', 'the'):\n","    ('<s>', 'the', 'whole'), Count: 6, Probability: 0.026432\n","    ('<s>', 'the', 'miss'), Count: 5, Probability: 0.022026\n","    ('<s>', 'the', 'next'), Count: 5, Probability: 0.022026\n","    ('<s>', 'the', 'house'), Count: 4, Probability: 0.017621\n","    ('<s>', 'the', 'morning'), Count: 4, Probability: 0.017621\n","  Top 5 Trigrams starting with ('of', 'the'):\n","    ('of', 'the', 'house'), Count: 17, Probability: 0.039535\n","    ('of', 'the', 'party'), Count: 13, Probability: 0.030233\n","    ('of', 'the', 'day'), Count: 12, Probability: 0.027907\n","    ('of', 'the', 'others'), Count: 11, Probability: 0.025581\n","    ('of', 'the', 'room'), Count: 11, Probability: 0.025581\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"353f8414","executionInfo":{"status":"ok","timestamp":1770286467712,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"bc9f1d5b-cc9f-4eba-d441-f65719785e25"},"source":["vocab_size = len(set(all_processed_words))\n","\n","print(f\"Vocabulary size (V): {vocab_size}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size (V): 7269\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6c3fa980","executionInfo":{"status":"ok","timestamp":1770286467714,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"f76c6685-f70c-4d4b-8d19-d32f89df2024"},"source":["def get_bigram_prob_smoothed(prev_word, current_word, vocab_size):\n","    # P_smoothed(current_word | prev_word) = (Count(prev_word, current_word) + 1) / (Count(prev_word) + V)\n","    numerator = bigram_counts[prev_word][current_word] + 1\n","    denominator = bigram_prefix_counts[prev_word] + vocab_size\n","    if denominator == 0: # Should not happen if vocab_size > 0, but as a safeguard\n","        return 0.0\n","    return numerator / denominator\n","\n","def get_trigram_prob_smoothed(prev_word1, prev_word2, current_word, vocab_size):\n","    # P_smoothed(current_word | prev_word1, prev_word2) = (Count(prev_word1, prev_word2, current_word) + 1) / (Count(prev_word1, prev_word2) + V)\n","    numerator = trigram_counts[prev_word1][prev_word2][current_word] + 1\n","    denominator = trigram_prefix_counts[prev_word1][prev_word2] + vocab_size\n","    if denominator == 0: # Should not happen if vocab_size > 0, but as a safeguard\n","        return 0.0\n","    return numerator / denominator\n","\n","print(\"Smoothed probability calculation functions defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Smoothed probability calculation functions defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edca24a5","executionInfo":{"status":"ok","timestamp":1770286467716,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"edd88745-cf4e-42e0-a30e-b09da96072a5"},"source":["print(\"\\n--- Example Smoothed Probabilities ---\")\n","\n","# Example Smoothed Bigram Probabilities\n","print(\"\\nSmoothed Bigram Examples:\")\n","# Example 1: A common bigram ('the', 'house')\n","prob_the_house = get_bigram_prob_smoothed('the', 'house', vocab_size)\n","print(f\"  P_smoothed('house' | 'the'): {prob_the_house:.6f} (Original: {get_bigram_prob('the', 'house'):.6f})\")\n","\n","# Example 2: An unlikely or unseen bigram ('banana', 'apple')\n","prob_unseen_bigram = get_bigram_prob_smoothed('banana', 'apple', vocab_size)\n","print(f\"  P_smoothed('apple' | 'banana'): {prob_unseen_bigram:.6f} (Original: {get_bigram_prob('banana', 'apple'):.6f})\")\n","\n","# Example 3: A bigram where the prev_word has low count ('hermione', 'granger') - assuming 'hermione' might be rare if not in a particular book\n","# Let's use a word known to be less frequent or potentially unseen in 'austen-sense.txt' and an arbitrary following word\n","prob_rare_prev = get_bigram_prob_smoothed('quizzical', 'look', vocab_size)\n","print(f\"  P_smoothed('look' | 'quizzical'): {prob_rare_prev:.6f} (Original: {get_bigram_prob('quizzical', 'look'):.6f})\")\n","\n","# Example Smoothed Trigram Probabilities\n","print(\"\\nSmoothed Trigram Examples:\")\n","# Example 1: A common trigram ('of', 'the', 'house')\n","prob_of_the_house = get_trigram_prob_smoothed('of', 'the', 'house', vocab_size)\n","print(f\"  P_smoothed('house' | 'of', 'the'): {prob_of_the_house:.6f} (Original: {get_trigram_prob('of', 'the', 'house'):.6f})\")\n","\n","# Example 2: An unlikely or unseen trigram ('i', 'am', 'spiderman')\n","prob_unseen_trigram = get_trigram_prob_smoothed('i', 'am', 'spiderman', vocab_size)\n","print(f\"  P_smoothed('spiderman' | 'i', 'am'): {prob_unseen_trigram:.6f} (Original: {get_trigram_prob('i', 'am', 'spiderman'):.6f})\")\n","\n","# Example 3: A trigram where the prefix has low count or is unseen ('very', 'unlikely', 'word')\n","# Let's use a less common bigram prefix and a arbitrary word\n","prob_rare_prefix = get_trigram_prob_smoothed('never', 'theless', 'said', vocab_size)\n","print(f\"  P_smoothed('said' | 'never', 'theless'): {prob_rare_prefix:.6f} (Original: {get_trigram_prob('never', 'theless', 'said'):.6f})\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Example Smoothed Probabilities ---\n","\n","Smoothed Bigram Examples:\n","  P_smoothed('house' | 'the'): 0.007573 (Original: 0.020798)\n","  P_smoothed('apple' | 'banana'): 0.000138 (Original: 0.000000)\n","  P_smoothed('look' | 'quizzical'): 0.000138 (Original: 0.000000)\n","\n","Smoothed Trigram Examples:\n","  P_smoothed('house' | 'of', 'the'): 0.002338 (Original: 0.039535)\n","  P_smoothed('spiderman' | 'i', 'am'): 0.000134 (Original: 0.000000)\n","  P_smoothed('said' | 'never', 'theless'): 0.000138 (Original: 0.000000)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4736e1d0","executionInfo":{"status":"ok","timestamp":1770286467724,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"f18b9a46-0e4d-4ce2-cc4f-30a75da65e9e"},"source":["test_sentences_raw = [\n","    \"She was a young woman of decided character.\",\n","    \"Her brother had lived long in the country.\",\n","    \"The sun shone brightly this morning.\",\n","    \"Never before had he seen such a sight.\",\n","    \"This is an extremely rare and unique occurrence.\"\n","]\n","\n","def preprocess_single_sentence(sentence_string):\n","    # Convert to lowercase\n","    sentence_string = sentence_string.lower()\n","\n","    # Remove punctuation and numbers, and replace multiple spaces with single space\n","    cleaned_sentence = re.sub(r'[^a-z\\s]', '', sentence_string)\n","    cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n","\n","    # Tokenize the cleaned sentence into words\n","    words = nltk.word_tokenize(cleaned_sentence)\n","\n","    # Add start and end of sentence tokens if the sentence is not empty\n","    if words:\n","        return ['<s>'] + words + ['</s>']\n","    else:\n","        return []\n","\n","# Preprocess all test sentences\n","preprocessed_test_sentences = [preprocess_single_sentence(s) for s in test_sentences_raw]\n","\n","print(\"Original test sentences:\")\n","for i, s in enumerate(test_sentences_raw):\n","    print(f\"  {i+1}. {s}\")\n","\n","print(\"\\nPreprocessed test sentences:\")\n","for i, ps in enumerate(preprocessed_test_sentences):\n","    print(f\"  {i+1}. {ps}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original test sentences:\n","  1. She was a young woman of decided character.\n","  2. Her brother had lived long in the country.\n","  3. The sun shone brightly this morning.\n","  4. Never before had he seen such a sight.\n","  5. This is an extremely rare and unique occurrence.\n","\n","Preprocessed test sentences:\n","  1. ['<s>', 'she', 'was', 'a', 'young', 'woman', 'of', 'decided', 'character', '</s>']\n","  2. ['<s>', 'her', 'brother', 'had', 'lived', 'long', 'in', 'the', 'country', '</s>']\n","  3. ['<s>', 'the', 'sun', 'shone', 'brightly', 'this', 'morning', '</s>']\n","  4. ['<s>', 'never', 'before', 'had', 'he', 'seen', 'such', 'a', 'sight', '</s>']\n","  5. ['<s>', 'this', 'is', 'an', 'extremely', 'rare', 'and', 'unique', 'occurrence', '</s>']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca97900e","executionInfo":{"status":"ok","timestamp":1770286467724,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"23c1af4d-72cf-4718-bd88-e4bb7d3b842a"},"source":["def calculate_sentence_probability_unigram(sentence_tokens):\n","    sentence_prob = 1.0\n","    for word in sentence_tokens:\n","        # Skip start token for unigram probability calculation as it doesn't represent a 'word' in the sentence content\n","        if word == '<s>':\n","            continue\n","        prob = get_unigram_prob(word)\n","        if prob == 0:\n","            # If a word has 0 unigram probability, the sentence probability is 0\n","            return 0.0\n","        sentence_prob *= prob\n","    return sentence_prob\n","\n","print(\"Unigram sentence probability function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unigram sentence probability function defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fe94d40","executionInfo":{"status":"ok","timestamp":1770286467725,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"c455904a-9ddc-415f-e68b-baf5ca0ef6bd"},"source":["def calculate_sentence_probability_bigram_smoothed(sentence_tokens, vocab_size):\n","    sentence_prob = 1.0\n","    # Bigram probabilities P(w_i | w_{i-1})\n","    # The sequence starts with <s>, so the first 'word' (w1) has context <s>\n","    # P(w1|<s>) * P(w2|w1) * ... * P(</s>|wn)\n","\n","    # Iterate from the second token, as the first token (index 0) is '<s>'\n","    # The loop should go up to and including the '</s>' token\n","    for i in range(1, len(sentence_tokens)):\n","        prev_word = sentence_tokens[i-1]\n","        current_word = sentence_tokens[i]\n","        prob = get_bigram_prob_smoothed(prev_word, current_word, vocab_size)\n","        sentence_prob *= prob\n","    return sentence_prob\n","\n","print(\"Smoothed Bigram sentence probability function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Smoothed Bigram sentence probability function defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"de6c1873","executionInfo":{"status":"ok","timestamp":1770286467725,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"e8cf1ad1-6d38-47e3-c1e8-32525cc0ff08"},"source":["def calculate_sentence_probability_trigram_smoothed(sentence_tokens, vocab_size):\n","    sentence_prob = 1.0\n","    # Trigram probabilities P(w_i | w_{i-2}, w_{i-1})\n","    # For the first word (index 1 after '<s>'), context is ('<s>', '<s>')\n","    # For the second word (index 2), context is ('<s>', w1)\n","    # For subsequent words, context is (w_{i-2}, w_{i-1})\n","\n","    # Special handling for the first word (P(w1 | <s>, <s>))\n","    # Assuming '<s>' is at index 0, and w1 at index 1\n","    # For trigrams, we effectively consider '<s> <s>' as the start context for the first actual word.\n","    # However, our current trigram_prefix_counts are based on (prev_word1, prev_word2).\n","    # The standard way to handle sentence start for trigrams is to imagine an implicit '<s>' before the first explicit '<s>'.\n","    # For simplicity and consistency with bigram, we'll use (<s>, <s>) for the first word, and (<s>, w1) for the second word.\n","    # This means the sentence_tokens array should conceptually be extended with an extra <s> at the beginning for the first trigram.\n","\n","    # The loop should start from the third token (index 2) to get the first full trigram (<s>, w1, w2) or (<s>, <s>, w1).\n","    # However, N-gram models for sentence probability typically calculate:\n","    # P(S) = P(w1|<s>) * P(w2|w1) * ... for bigram\n","    # P(S) = P(w1|<s>,<s>) * P(w2|<s>,w1) * P(w3|w1,w2) * ... for trigram\n","\n","    # Let's adjust the `sentence_tokens` to include an extra '<s>' at the very beginning to simplify the loop for trigrams.\n","    # This allows `sentence_tokens[i-2]` to correctly reference '<s>' for the first actual word.\n","    extended_tokens = ['<s>'] + sentence_tokens\n","\n","    # Iterate from the third token (index 2 of extended_tokens, which is the actual first word w1)\n","    # up to and including the '</s>' token.\n","    # extended_tokens[i-2], extended_tokens[i-1], extended_tokens[i]\n","    for i in range(2, len(extended_tokens)):\n","        prev_word1 = extended_tokens[i-2]\n","        prev_word2 = extended_tokens[i-1]\n","        current_word = extended_tokens[i]\n","        prob = get_trigram_prob_smoothed(prev_word1, prev_word2, current_word, vocab_size)\n","        sentence_prob *= prob\n","    return sentence_prob\n","\n","print(\"Smoothed Trigram sentence probability function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Smoothed Trigram sentence probability function defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0de0f3da","executionInfo":{"status":"ok","timestamp":1770286467725,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"a4bdf6ed-f186-4ecb-b38b-ddd57b0dd761"},"source":["print(\"\\n--- Sentence Probabilities ---\")\n","\n","for i, sentence_tokens in enumerate(preprocessed_test_sentences):\n","    print(f\"\\nSentence {i+1}: {' '.join(sentence_tokens)}\")\n","\n","    # Unigram Probability\n","    prob_uni = calculate_sentence_probability_unigram(sentence_tokens)\n","    print(f\"  Unigram Probability: {prob_uni:.15e}\")\n","\n","    # Bigram Smoothed Probability\n","    prob_bi_smoothed = calculate_sentence_probability_bigram_smoothed(sentence_tokens, vocab_size)\n","    print(f\"  Smoothed Bigram Probability: {prob_bi_smoothed:.15e}\")\n","\n","    # Trigram Smoothed Probability\n","    prob_tri_smoothed = calculate_sentence_probability_trigram_smoothed(sentence_tokens, vocab_size)\n","    print(f\"  Smoothed Trigram Probability: {prob_tri_smoothed:.15e}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Sentence Probabilities ---\n","\n","Sentence 1: <s> she was a young woman of decided character </s>\n","  Unigram Probability: 1.676744619701078e-23\n","  Smoothed Bigram Probability: 1.744529891976736e-25\n","  Smoothed Trigram Probability: 2.764223128567893e-32\n","\n","Sentence 2: <s> her brother had lived long in the country </s>\n","  Unigram Probability: 4.364323552540476e-23\n","  Smoothed Bigram Probability: 6.958090069712478e-27\n","  Smoothed Trigram Probability: 2.581581728896265e-33\n","\n","Sentence 3: <s> the sun shone brightly this morning </s>\n","  Unigram Probability: 0.000000000000000e+00\n","  Smoothed Bigram Probability: 4.273736829635853e-23\n","  Smoothed Trigram Probability: 3.609335002382160e-27\n","\n","Sentence 4: <s> never before had he seen such a sight </s>\n","  Unigram Probability: 1.379481532173963e-23\n","  Smoothed Bigram Probability: 1.436900280542194e-30\n","  Smoothed Trigram Probability: 1.730085901927089e-35\n","\n","Sentence 5: <s> this is an extremely rare and unique occurrence </s>\n","  Unigram Probability: 0.000000000000000e+00\n","  Smoothed Bigram Probability: 8.023018338880567e-32\n","  Smoothed Trigram Probability: 3.487032344347986e-34\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9301e7e","executionInfo":{"status":"ok","timestamp":1770286467736,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"22027798-0c61-4b6e-def6-4e17491f5677"},"source":["import math\n","\n","def calculate_perplexity(sentence_probability, num_words):\n","    # Perplexity is (1 / P(sentence))^(1/N)\n","    # If probability is zero, perplexity is infinite\n","    if sentence_probability == 0:\n","        return float('inf')\n","    # Use log to avoid underflow for very small probabilities\n","    # log(Perplexity) = (-1/N) * log(P(sentence))\n","    # Perplexity = exp((-1/N) * log(P(sentence)))\n","\n","    # The length N should correspond to the words being predicted.\n","    # For a sentence s = w1, w2, ..., wn, it includes wn, but excludes <s>.\n","    # For our case, sentence_tokens includes <s> and </s>. So num_words will be len(sentence_tokens) - 1 (excluding <s>)\n","\n","    try:\n","        # Ensure probability is positive for log. If it's effectively zero but not exactly 0.0 due to floating point, handle it.\n","        if sentence_probability <= 0:\n","            return float('inf')\n","        return math.exp(-math.log(sentence_probability) / num_words)\n","    except OverflowError:\n","        return float('inf') # Handles cases where exp argument is too large/small\n","\n","print(\"Perplexity calculation function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Perplexity calculation function defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ca950f5","executionInfo":{"status":"ok","timestamp":1770286467743,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anudeep","userId":"03657578648993592904"}},"outputId":"22cf6d88-a2ae-4102-d1b4-c0eaf790a6c7"},"source":["print(\"\\n--- Sentence Perplexities ---\")\n","\n","for i, sentence_tokens in enumerate(preprocessed_test_sentences):\n","    print(f\"\\nSentence {i+1}: {' '.join(sentence_tokens)}\")\n","\n","    # Number of words for perplexity calculation (excluding <s>, including </s>)\n","    # This is consistent for unigram, bigram, and trigram calculations based on common practice\n","    num_words_for_perplexity = len(sentence_tokens) - 1 # Excludes the first <s>\n","\n","    # Unigram Perplexity\n","    prob_uni = calculate_sentence_probability_unigram(sentence_tokens)\n","    ppl_uni = calculate_perplexity(prob_uni, num_words_for_perplexity)\n","    print(f\"  Unigram Perplexity: {ppl_uni:.2f}\")\n","\n","    # Bigram Smoothed Perplexity\n","    prob_bi_smoothed = calculate_sentence_probability_bigram_smoothed(sentence_tokens, vocab_size)\n","    ppl_bi_smoothed = calculate_perplexity(prob_bi_smoothed, num_words_for_perplexity)\n","    print(f\"  Smoothed Bigram Perplexity: {ppl_bi_smoothed:.2f}\")\n","\n","    # Trigram Smoothed Perplexity\n","    prob_tri_smoothed = calculate_sentence_probability_trigram_smoothed(sentence_tokens, vocab_size)\n","    ppl_tri_smoothed = calculate_perplexity(prob_tri_smoothed, num_words_for_perplexity)\n","    print(f\"  Smoothed Trigram Perplexity: {ppl_tri_smoothed:.2f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Sentence Perplexities ---\n","\n","Sentence 1: <s> she was a young woman of decided character </s>\n","  Unigram Perplexity: 339.32\n","  Smoothed Bigram Perplexity: 563.54\n","  Smoothed Trigram Perplexity: 3209.90\n","\n","Sentence 2: <s> her brother had lived long in the country </s>\n","  Unigram Perplexity: 305.11\n","  Smoothed Bigram Perplexity: 806.10\n","  Smoothed Trigram Perplexity: 4177.36\n","\n","Sentence 3: <s> the sun shone brightly this morning </s>\n","  Unigram Perplexity: inf\n","  Smoothed Bigram Perplexity: 1568.91\n","  Smoothed Trigram Perplexity: 5991.15\n","\n","Sentence 4: <s> never before had he seen such a sight </s>\n","  Unigram Perplexity: 346.76\n","  Smoothed Bigram Perplexity: 2069.39\n","  Smoothed Trigram Perplexity: 7285.12\n","\n","Sentence 5: <s> this is an extremely rare and unique occurrence </s>\n","  Unigram Perplexity: inf\n","  Smoothed Bigram Perplexity: 2851.50\n","  Smoothed Trigram Perplexity: 5218.01\n"]}]}]}